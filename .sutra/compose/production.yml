services:
  # ============================================================================
  # STORAGE LAYER
  # ============================================================================
  
  # Main Storage Server (Core Sutra AI Knowledge Graph)
  storage-server:
    build:
      context: ../..
      dockerfile: ./packages/sutra-storage/Dockerfile
    image: sutra-storage-server:${SUTRA_VERSION:-latest}
    container_name: sutra-storage
    ports:
      - "50051:50051"
    volumes:
      - storage-data:/data
    environment:
      - RUST_LOG=info
      - STORAGE_PATH=/data
      - STORAGE_HOST=0.0.0.0
      - STORAGE_PORT=50051
      - VECTOR_DIMENSION=768
      # Edition Configuration
      - SUTRA_EDITION=${SUTRA_EDITION:-simple}
      - SUTRA_LICENSE_KEY=${SUTRA_LICENSE_KEY}
      # Sharded Storage Configuration (edition-dependent)
      - SUTRA_STORAGE_MODE=${SUTRA_STORAGE_MODE:-single}
      - SUTRA_NUM_SHARDS=${SUTRA_NUM_SHARDS:-1}
      # Unified Learning Pipeline Configuration (edition-dependent)
      - SUTRA_EMBEDDING_SERVICE_URL=${SUTRA_EMBEDDING_URL:-http://embedding-single:8888}
      - SUTRA_EMBEDDING_TIMEOUT_SEC=30
      - SUTRA_MIN_ASSOCIATION_CONFIDENCE=0.5
      - SUTRA_MAX_ASSOCIATIONS_PER_CONCEPT=10
    depends_on:
      embedding-single:
        condition: service_healthy
    networks:
      - sutra-network
    restart: unless-stopped
    healthcheck:
      test: ["CMD-SHELL", "timeout 1 bash -c '</dev/tcp/localhost/50051' || exit 1"]
      interval: 30s
      timeout: 5s
      retries: 3
      start_period: 10s

  # Grid Event Storage (Reserved for Grid Observability - Enterprise Only)
  grid-event-storage:
    build:
      context: ../..
      dockerfile: ./packages/sutra-storage/Dockerfile
    image: sutra-storage-server:${SUTRA_VERSION:-latest}
    container_name: sutra-grid-events
    ports:
      - "50052:50051"
    volumes:
      - grid-event-data:/data
    environment:
      - RUST_LOG=info
      - STORAGE_PATH=/data
      - STORAGE_HOST=0.0.0.0
      - STORAGE_PORT=50051
      - VECTOR_DIMENSION=768
      # Edition Configuration
      - SUTRA_EDITION=enterprise
      # Use HA embedding service (same as main storage)
      - SUTRA_EMBEDDING_SERVICE_URL=http://embedding-ha:8888
      - SUTRA_EMBEDDING_TIMEOUT_SEC=30
      - SUTRA_MIN_ASSOCIATION_CONFIDENCE=0.5
      - SUTRA_MAX_ASSOCIATIONS_PER_CONCEPT=10
    depends_on:
      embedding-ha:
        condition: service_healthy
    networks:
      - sutra-network
    restart: unless-stopped
    healthcheck:
      test: ["CMD-SHELL", "timeout 1 bash -c '</dev/tcp/localhost/50051' || exit 1"]
      interval: 30s
      timeout: 5s
      retries: 3
      start_period: 10s
    profiles:
      - enterprise

  # User Storage (User Management, Auth, Conversations - All Editions)
  user-storage-server:
    build:
      context: ../..
      dockerfile: ./packages/sutra-storage/Dockerfile
    image: sutra-storage-server:${SUTRA_VERSION:-latest}
    container_name: sutra-user-storage
    ports:
      - "50053:50051"
    volumes:
      - user-storage-data:/data
    environment:
      - RUST_LOG=info
      - STORAGE_PATH=/data
      - STORAGE_HOST=0.0.0.0
      - STORAGE_PORT=50051
      - VECTOR_DIMENSION=768
      # Edition Configuration
      - SUTRA_EDITION=${SUTRA_EDITION:-simple}
      - SUTRA_LICENSE_KEY=${SUTRA_LICENSE_KEY}
      # Unified Learning Pipeline Configuration
      - SUTRA_EMBEDDING_SERVICE_URL=${SUTRA_EMBEDDING_URL:-http://embedding-single:8888}
      - SUTRA_EMBEDDING_TIMEOUT_SEC=30
      - SUTRA_MIN_ASSOCIATION_CONFIDENCE=0.5
      - SUTRA_MAX_ASSOCIATIONS_PER_CONCEPT=10
      - SUTRA_SEMANTIC_ANALYSIS=false  # Disable for user storage (auth data only)
    depends_on:
      embedding-single:
        condition: service_healthy
    networks:
      - sutra-network
    restart: unless-stopped
    healthcheck:
      test: ["CMD-SHELL", "timeout 1 bash -c '</dev/tcp/localhost/50051' || exit 1"]
      interval: 30s
      timeout: 5s
      retries: 3
      start_period: 10s

  # ============================================================================
  # GRID INFRASTRUCTURE
  # ============================================================================
  
  # Grid Master (Orchestration & Coordination - Enterprise Only)
  grid-master:
    build:
      context: ../..
      dockerfile: ./packages/sutra-grid-master/Dockerfile
    image: sutra-grid-master:${SUTRA_VERSION:-latest}
    container_name: sutra-grid-master
    ports:
      - "7001:7001"  # HTTP binary distribution
      - "7002:7002"  # TCP agent connections
    environment:
      - RUST_LOG=info
      - GRID_MASTER_HOST=0.0.0.0
      - GRID_MASTER_TCP_PORT=7002
      - GRID_MASTER_HTTP_PORT=7001
      - EVENT_STORAGE=grid-event-storage:50051
      # Edition enforcement
      - SUTRA_EDITION=enterprise
      - SUTRA_LICENSE_KEY=${SUTRA_LICENSE_KEY}
    depends_on:
      - grid-event-storage
    networks:
      - sutra-network
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "nc", "-z", "localhost", "7002"]
      interval: 30s
      timeout: 5s
      retries: 3
      start_period: 15s
    profiles:
      - enterprise

  # Grid Agent 1 (Node Management - Enterprise Only)
  grid-agent-1:
    build:
      context: ../..
      dockerfile: ./packages/sutra-grid-agent/Dockerfile
    image: sutra-grid-agent:${SUTRA_VERSION:-latest}
    container_name: sutra-grid-agent-1
    ports:
      - "8003:8001"
    volumes:
      - agent1-data:/storage-nodes
    environment:
      - RUST_LOG=info
      - GRID_AGENT_HOST=0.0.0.0
      - GRID_AGENT_PORT=8001
      - GRID_MASTER_ADDRESS=grid-master:7002
      - EVENT_STORAGE=grid-event-storage:50051
      - AGENT_ID=agent-001
      - MAX_STORAGE_NODES=5
      # Edition enforcement
      - SUTRA_EDITION=enterprise
    depends_on:
      - grid-master
      - grid-event-storage
    networks:
      - sutra-network
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "nc", "-z", "grid-master", "7002"]
      interval: 30s
      timeout: 5s
      retries: 3
      start_period: 20s
    profiles:
      - enterprise

  # Grid Agent 2 (Additional Node Management - Enterprise Only)
  grid-agent-2:
    build:
      context: ../..
      dockerfile: ./packages/sutra-grid-agent/Dockerfile
    image: sutra-grid-agent:${SUTRA_VERSION:-latest}
    container_name: sutra-grid-agent-2
    ports:
      - "8004:8001"
    volumes:
      - agent2-data:/storage-nodes
    environment:
      - RUST_LOG=info
      - GRID_AGENT_HOST=0.0.0.0
      - GRID_AGENT_PORT=8001
      - GRID_MASTER_ADDRESS=grid-master:7002
      - EVENT_STORAGE=grid-event-storage:50051
      - AGENT_ID=agent-002
      - MAX_STORAGE_NODES=5
      # Edition enforcement
      - SUTRA_EDITION=enterprise
    depends_on:
      - grid-master
      - grid-event-storage
    networks:
      - sutra-network
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "nc", "-z", "grid-master", "7002"]
      interval: 30s
      timeout: 5s
      retries: 3
      start_period: 20s
    profiles:
      - enterprise

  # ============================================================================
  # API LAYER
  # ============================================================================
  
  # Sutra API (Primary REST API)
  sutra-api:
    build:
      context: ../..
      dockerfile: ./packages/sutra-api/Dockerfile
    image: sutra-api:${SUTRA_VERSION:-latest}
    container_name: sutra-api
    ports:
      - "8000:8000"
    environment:
      - PYTHONUNBUFFERED=1
      - SUTRA_STORAGE_MODE=server
      - SUTRA_STORAGE_SERVER=storage-server:50051
      - SUTRA_USER_STORAGE_SERVER=user-storage-server:50051
      # JWT Configuration
      - SUTRA_JWT_SECRET_KEY=${SUTRA_JWT_SECRET_KEY:-INSECURE_DEFAULT_SECRET_CHANGE_IN_PRODUCTION}
      - SUTRA_JWT_ALGORITHM=HS256
      - SUTRA_JWT_EXPIRATION_HOURS=24
      - SUTRA_JWT_REFRESH_EXPIRATION_DAYS=7
      # Edition Configuration
      - SUTRA_EDITION=${SUTRA_EDITION:-simple}
      - SUTRA_LICENSE_KEY=${SUTRA_LICENSE_KEY}
      - SUTRA_LICENSE_SECRET=${SUTRA_LICENSE_SECRET}
      # Rate limits are set by feature flags, but keep for backwards compatibility
      - SUTRA_RATE_LIMIT_LEARN=100
      - SUTRA_RATE_LIMIT_REASON=200
    depends_on:
      - storage-server
      - user-storage-server
    networks:
      - sutra-network
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s
      timeout: 3s
      retries: 3
      start_period: 10s

  # ============================================================================
  # ML FOUNDATION SERVICES
  # ============================================================================
  
  # ML-Base Service (Centralized ML Inference Platform)
  ml-base-service:
    build:
      context: ../../packages/sutra-ml-base-service
      dockerfile: Dockerfile
    image: sutra-ml-base-service:${SUTRA_VERSION:-latest}
    container_name: sutra-ml-base
    ports:
      - "8887:8887"  # ML inference endpoint
    environment:
      - PYTHONUNBUFFERED=1
      - ML_BASE_PORT=8887
      - ML_BASE_HOST=0.0.0.0
      # Edition Configuration
      - SUTRA_EDITION=${SUTRA_EDITION:-simple}
      - SUTRA_LICENSE_KEY=${SUTRA_LICENSE_KEY}
      # Model Configuration
      - ML_BASE_EMBEDDING_MODEL=nomic-ai/nomic-embed-text-v1.5
      - ML_BASE_NLG_MODEL=google/gemma-2-2b-it
      - ML_BASE_CACHE_SIZE=10000
      - ML_BASE_MODEL_CACHE_DIR=/models/cache
      # Performance Configuration
      - ML_BASE_MAX_BATCH_SIZE=64
      - ML_BASE_BATCH_TIMEOUT_MS=50
      - ML_BASE_MODEL_UNLOAD_TIMEOUT=300
      # Monitoring
      - ML_BASE_METRICS_ENABLED=true
      - ML_BASE_LOG_LEVEL=INFO
    volumes:
      - ml-models-cache:/models/cache
      - /var/log/sutra:/var/log/sutra
    networks:
      - sutra-network
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8887/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s
    deploy:
      resources:
        limits:
          memory: 6G
        reservations:
          memory: 4G

  # ============================================================================
  # EMBEDDING SERVICES (Lightweight Clients)
  # ============================================================================
  
  # Single Embedding Service (Simple & Community Editions) - Lightweight Client
  embedding-single:
    build:
      context: ../../packages/sutra-embedding-service
      dockerfile: Dockerfile
    image: sutra-embedding-service:${SUTRA_VERSION:-latest}
    container_name: embedding-single
    ports:
      - "8888:8888"  # Client endpoint (proxies to ML-Base)
    environment:
      - PYTHONUNBUFFERED=1
      - EMBEDDING_PORT=8888
      - EMBEDDING_HOST=0.0.0.0
      # ML-Base Service Configuration
      - ML_BASE_URL=http://ml-base-service:8887
      - ML_BASE_TIMEOUT=30
      - ML_BASE_MAX_RETRIES=3
      # Edition Configuration
      - SUTRA_EDITION=${SUTRA_EDITION:-simple}
      - SUTRA_LICENSE_KEY=${SUTRA_LICENSE_KEY}
      # Cache Configuration
      - EMBEDDING_CACHE_SIZE=1000
      - EMBEDDING_CACHE_TTL=3600
      # Instance Configuration
      - INSTANCE_ID=embedding-single
    depends_on:
      ml-base-service:
        condition: service_healthy
    networks:
      - sutra-network
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8888/health"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 30s
    deploy:
      resources:
        limits:
          memory: 256M
        reservations:
          memory: 128M
    profiles:
      - simple
      - community
  
  # ============================================================================
  # EMBEDDING SERVICE HA (Enterprise Edition Only)
  # ============================================================================
  
  # Embedding Service Replica 1
  embedding-1:
    image: sutra-embedding-service:latest
    container_name: embedding-1
    environment:
      - RUST_LOG=info
      - EMBEDDING_PORT=8888
      - MODEL_PATH=/models/nomic-embed-text-v1.5
      - CACHE_SIZE=10000
      - BATCH_SIZE=32
      - EVENT_STORAGE=grid-event-storage:50051
      - INSTANCE_ID=embedding-1
    volumes:
      - ../../packages/sutra-embedding-service/models:/models:ro
    networks:
      - sutra-network
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "python", "-c", "import urllib.request; urllib.request.urlopen('http://localhost:8888/health').read()"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 30s
    deploy:
      resources:
        limits:
          memory: 2G
        reservations:
          memory: 1G
    profiles:
      - enterprise
  
  # Embedding Service Replica 2
  embedding-2:
    image: sutra-embedding-service:latest
    container_name: embedding-2
    environment:
      - RUST_LOG=info
      - EMBEDDING_PORT=8888
      - MODEL_PATH=/models/nomic-embed-text-v1.5
      - CACHE_SIZE=10000
      - BATCH_SIZE=32
      - EVENT_STORAGE=grid-event-storage:50051
      - INSTANCE_ID=embedding-2
    volumes:
      - ../../packages/sutra-embedding-service/models:/models:ro
    networks:
      - sutra-network
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "python", "-c", "import urllib.request; urllib.request.urlopen('http://localhost:8888/health').read()"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 30s
    deploy:
      resources:
        limits:
          memory: 2G
        reservations:
          memory: 1G
    profiles:
      - enterprise
  
  # Embedding Service Replica 3
  embedding-3:
    image: sutra-embedding-service:latest
    container_name: embedding-3
    environment:
      - RUST_LOG=info
      - EMBEDDING_PORT=8888
      - MODEL_PATH=/models/nomic-embed-text-v1.5
      - CACHE_SIZE=10000
      - BATCH_SIZE=32
      - EVENT_STORAGE=grid-event-storage:50051
      - INSTANCE_ID=embedding-3
    volumes:
      - ../../packages/sutra-embedding-service/models:/models:ro
    networks:
      - sutra-network
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "python", "-c", "import urllib.request; urllib.request.urlopen('http://localhost:8888/health').read()"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 30s
    deploy:
      resources:
        limits:
          memory: 2G
        reservations:
          memory: 1G
    profiles:
      - enterprise
  
  # HAProxy Load Balancer for Embedding Services
  embedding-ha:
    image: haproxy:2.8-alpine
    container_name: embedding-ha
    ports:
      - "8888:8888"  # Embedding service endpoint
      - "8404:8404"  # HAProxy stats
    volumes:
      - ./docker/haproxy.cfg:/usr/local/etc/haproxy/haproxy.cfg:ro
    depends_on:
      - embedding-1
      - embedding-2
      - embedding-3
    networks:
      - sutra-network
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "haproxy", "-c", "-f", "/usr/local/etc/haproxy/haproxy.cfg"]
      interval: 10s
      timeout: 3s
      retries: 3
      start_period: 5s
    profiles:
      - enterprise
  

  # ============================================================================
  # NLG SERVICES
  # ============================================================================
  
  # NLG Service (Simple/Community Edition) - Lightweight Client
  nlg-single:
    build:
      context: ../../packages/sutra-nlg-service
      dockerfile: Dockerfile
    image: sutra-nlg-service:${SUTRA_VERSION:-latest}
    container_name: nlg-single
    ports:
      - "8003:8003"  # Client endpoint (proxies to ML-Base)
    environment:
      - PYTHONUNBUFFERED=1
      - NLG_PORT=8003
      - NLG_HOST=0.0.0.0
      # ML-Base Service Configuration
      - ML_BASE_URL=http://ml-base-service:8887
      - ML_BASE_TIMEOUT=60
      - ML_BASE_MAX_RETRIES=3
      # Edition Configuration
      - SUTRA_EDITION=${SUTRA_EDITION:-simple}
      - SUTRA_LICENSE_KEY=${SUTRA_LICENSE_KEY}
      # Cache Configuration
      - NLG_CACHE_SIZE=500
      - NLG_CACHE_TTL=1800
      # Instance Configuration
      - INSTANCE_ID=nlg-single
    depends_on:
      ml-base-service:
        condition: service_healthy
    networks:
      - sutra-network
    restart: "on-failure:3"  # NLG is optional - stop after 3 failed attempts
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8003/health"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 30s
    deploy:
      resources:
        limits:
          memory: 256M
        reservations:
          memory: 128M
    profiles:
      - simple
      - community
  
  # ============================================================================
  # NLG SERVICE HA (Enterprise Edition Only)
  # ============================================================================
  
  # NLG Service Replica 1
  nlg-ha-1:
    build:
      context: ../../packages/sutra-nlg-service
      dockerfile: Dockerfile
    image: sutra-nlg-service:${SUTRA_VERSION:-latest}
    container_name: nlg-1
    env_file:
      - .env
    environment:
      - NLG_MODEL=${SUTRA_NLG_MODEL:-google/gemma-3-270m-it}
      - INSTANCE_ID=nlg-1
      - PORT=8889
      - HF_TOKEN=${HF_TOKEN}
    volumes:
      - ../../packages/sutra-nlg-service/models:/app/models:ro
    networks:
      - sutra-network
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "python", "-c", "import urllib.request; urllib.request.urlopen('http://localhost:8889/health').read()"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 120s
    deploy:
      resources:
        limits:
          memory: 5G
        reservations:
          memory: 3G
    profiles:
      - nlg-hybrid  # Optional: Enable with SUTRA_NLG_ENABLED=true
  
  # NLG Service Replica 2
  nlg-2:
    image: sutra-nlg-service:latest
    container_name: nlg-2
    env_file:
      - .env
    environment:
      - NLG_MODEL=${SUTRA_NLG_MODEL:-google/gemma-3-270m-it}
      - INSTANCE_ID=nlg-2
      - PORT=8889
      - HF_TOKEN=${HF_TOKEN}
    volumes:
      - ../../packages/sutra-nlg-service/models:/app/models:ro
    networks:
      - sutra-network
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "python", "-c", "import urllib.request; urllib.request.urlopen('http://localhost:8889/health').read()"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 120s
    deploy:
      resources:
        limits:
          memory: 5G
        reservations:
          memory: 3G
    profiles:
      - enterprise
  
  # NLG Service Replica 3
  nlg-3:
    image: sutra-nlg-service:latest
    container_name: nlg-3
    env_file:
      - .env
    environment:
      - NLG_MODEL=${SUTRA_NLG_MODEL:-google/gemma-3-270m-it}
      - INSTANCE_ID=nlg-3
      - PORT=8889
      - HF_TOKEN=${HF_TOKEN}
    volumes:
      - ../../packages/sutra-nlg-service/models:/app/models:ro
    networks:
      - sutra-network
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "python", "-c", "import urllib.request; urllib.request.urlopen('http://localhost:8889/health').read()"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 120s
    deploy:
      resources:
        limits:
          memory: 5G
        reservations:
          memory: 3G
    profiles:
      - enterprise
  
  # HAProxy Load Balancer for NLG Services
  nlg-ha:
    image: haproxy:2.8-alpine
    container_name: nlg-ha
    ports:
      - "8889:8889"  # NLG service endpoint
      - "8405:8405"  # HAProxy stats (different from embedding)
    volumes:
      - ./docker/haproxy-nlg.cfg:/usr/local/etc/haproxy/haproxy.cfg:ro
    depends_on:
      - nlg-1
      - nlg-2
      - nlg-3
    networks:
      - sutra-network
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "haproxy", "-c", "-f", "/usr/local/etc/haproxy/haproxy.cfg"]
      interval: 10s
      timeout: 3s
      retries: 3
      start_period: 5s
    profiles:
      - enterprise

  # Sutra Hybrid (Semantic Embeddings API + NLG)
  sutra-hybrid:
    build:
      context: ../..
      dockerfile: ./packages/sutra-hybrid/Dockerfile
    image: sutra-hybrid:${SUTRA_VERSION:-latest}
    container_name: sutra-hybrid
    ports:
      - "8001:8000"
    environment:
      - PYTHONUNBUFFERED=1
      - SUTRA_STORAGE_MODE=server
      - SUTRA_STORAGE_SERVER=storage-server:50051
      # Edition Configuration
      - SUTRA_EDITION=${SUTRA_EDITION:-simple}
      - SUTRA_LICENSE_KEY=${SUTRA_LICENSE_KEY}
      - SUTRA_LICENSE_SECRET=${SUTRA_LICENSE_SECRET}
      # Semantic Embeddings
      - SUTRA_USE_SEMANTIC_EMBEDDINGS=true
      - SUTRA_EMBEDDING_PROVIDER=service
      - SUTRA_EMBEDDING_SERVICE_URL=${SUTRA_EMBEDDING_URL:-http://embedding-single:8888}
      - SUTRA_EMBEDDING_MODEL=nomic-embed-text
      - SUTRA_VECTOR_DIMENSION=768
      # NLG Configuration (always enabled)
      - SUTRA_NLG_ENABLED=true
      - SUTRA_NLG_MODE=${SUTRA_NLG_MODE:-hybrid}
      - SUTRA_NLG_SERVICE_URL=${SUTRA_NLG_URL:-http://nlg-single:8889}
    depends_on:
      - storage-server
    networks:
      - sutra-network
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/ping"]
      interval: 30s
      timeout: 3s
      retries: 3
      start_period: 10s

  # ============================================================================
  # WEB INTERFACES
  # ============================================================================
  
  # Sutra Control Center (Grid Management + System Monitoring)
  sutra-control:
    build:
      context: ../..
      dockerfile: ./packages/sutra-control/Dockerfile
    image: sutra-control:${SUTRA_IMAGE_TAG:-latest}
    container_name: sutra-control
    ports:
      - "9000:9000"
    environment:
      - PYTHONUNBUFFERED=1
      - PYTHONDONTWRITEBYTECODE=1
      - ENVIRONMENT=production
      # Edition Configuration
      - SUTRA_EDITION=${SUTRA_EDITION:-simple}
      - SUTRA_LICENSE_KEY=${SUTRA_LICENSE_KEY}
      # Service URLs
      - SUTRA_STORAGE_SERVER=storage-server:50051
      - SUTRA_GRID_MASTER=${SUTRA_GRID_MASTER_URL:-}
      - PORT=9000
    depends_on:
      - storage-server
      - sutra-api
      - sutra-hybrid
    networks:
      - sutra-network
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:9000/health"]
      interval: 30s
      timeout: 3s
      retries: 3
      start_period: 15s


  # Sutra Client (Interactive AI Interface)
  # Sutra Client (Conversation-First UI)
  sutra-client:
    build:
      context: ../../packages/sutra-client
      dockerfile: Dockerfile
    image: sutra-client:${SUTRA_VERSION:-latest}
    container_name: sutra-client
    ports:
      - "8080:8080"
    environment:
      # Edition Configuration (for UI display)
      - SUTRA_EDITION=${SUTRA_EDITION:-simple}
      # Service URLs (nginx proxies internally, not exposed to client)
      - SUTRA_API_URL=http://sutra-api:8000
      - SUTRA_HYBRID_URL=http://sutra-hybrid:8001
    depends_on:
      sutra-api:
        condition: service_healthy
      user-storage-server:
        condition: service_healthy
      sutra-hybrid:
        condition: service_healthy
    networks:
      - sutra-network
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8080/health"]
      interval: 30s
      timeout: 3s
      retries: 3
      start_period: 10s

  # ============================================================================
  # BULK INGESTION LAYER (Optional - can be disabled)
  # ============================================================================
  
  # Primary Bulk Ingester Service
  sutra-bulk-ingester:
    build:
      context: ../..
      dockerfile: ./packages/sutra-bulk-ingester/Dockerfile
    image: sutra-bulk-ingester:${SUTRA_VERSION:-latest}
    container_name: sutra-bulk-ingester
    ports:
      - "8005:8005"  # Bulk ingestion API
    volumes:
      - ./datasets:/datasets:ro  # Read-only dataset access
      - ingestion-jobs:/jobs     # Job state persistence
    environment:
      - RUST_LOG=info
      # Edition Configuration
      - SUTRA_EDITION=${SUTRA_EDITION:-simple}
      - SUTRA_LICENSE_KEY=${SUTRA_LICENSE_KEY}
      # Service Configuration
      - SUTRA_STORAGE_MODE=server
      - SUTRA_STORAGE_SERVER=storage-server:50051
      - SUTRA_EMBEDDING_SERVICE_URL=${SUTRA_EMBEDDING_URL:-http://embedding-single:8888}
      - SUTRA_GRID_MASTER=${SUTRA_GRID_MASTER_URL:-}
      # Worker configuration (set by feature flags based on edition)
      - INGESTER_WORKERS=${SUTRA_INGEST_WORKERS:-2}
      - INGESTER_BATCH_SIZE=100
      - INGESTER_MEMORY_LIMIT=2048
    depends_on:
      storage-server:
        condition: service_healthy
    networks:
      - sutra-network
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8005/health"]
      interval: 30s
      timeout: 3s
      retries: 3
      start_period: 20s

# ============================================================================
# NETWORKING & STORAGE
# ============================================================================

volumes:
  # Docker managed volumes (easier for deployment)
  storage-data:
    driver: local
  grid-event-data:
    driver: local
  user-storage-data:
    driver: local
  agent1-data:
    driver: local
  agent2-data:
    driver: local
  # New volume for bulk ingestion (only created when bulk-ingester profile used)
  ingestion-jobs:
    driver: local
  # ML model cache volume
  ml-models-cache:
    driver: local

networks:
  sutra-network:
    driver: bridge
    ipam:
      driver: default
      config:
        - subnet: 172.20.0.0/16
