# Embedding Service v3.0 - Production Scaling with Sutra-Native Caching

**21Ã— Performance Improvement | 85% Cache Hit Rate | Zero External Dependencies**

Version: 3.0.0 | Architecture: Cached Client + Scaled ML-Base | Status: Production-Ready âœ…

---

## Overview

The **Embedding Service v3.0** introduces **production-grade scaling** with three optimization phases that deliver 21Ã— performance improvement while maintaining zero external dependencies:

**Phase 0: Matryoshka Dimensions (3Ã— faster)**
- Configurable 256/512/768-dim embeddings with layer normalization
- 67% storage reduction per concept (3KB â†’ 1KB for 256-dim)
- MATRYOSHKA_DIM environment variable for dimension selection

**Phase 1: Sutra-Native Caching (7Ã— total)**
- L1 in-memory LRU cache: 10K entries, 68% hit rate, ~2Âµs latency
- L2 Sutra Storage shard: 100K concepts, 17% additional hits, ~2ms latency  
- 100% Sutra-native (no Redis, Memcached, or external cache)
- WAL-backed persistence (survives restarts)

**Phase 2: HAProxy Load Balancing (21Ã— total)**
- 3Ã— ML-Base replicas with intelligent leastconn routing
- 1,500-3,000 concurrent user capacity
- Automatic health checks and failover

**Key Benefits:**
- ðŸš€ **21Ã— Throughput**: From 0.14 to 8.8 concepts/sec
- âš¡ **85% Cache Hit Rate**: Per-text caching eliminates redundant ML inference
- ðŸ”§ **Zero External Dependencies**: 100% Sutra-native (no Redis, Prometheus, PostgreSQL)
- ðŸ“Š **Production Metrics**: 50ms avg latency (cache hit), 700ms (cache miss)
- ðŸ—ï¸ **Flexible Deployment**: Deploy phases incrementally as needed

---

## ðŸ—ï¸ Architecture Evolution

### v1.x (Monolithic)
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚     Embedding Service (1.38GB)         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ â€¢ Full PyTorch Stack                   â”‚
â”‚ â€¢ nomic-embed-text-v1.5 Model (1.3GB)  â”‚
â”‚ â€¢ FastAPI Server                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```
**Challenge**: 1.38GB Ã— 3 replicas = 4.14GB for minimal scale

### v2.0 (ML-Base Client)  
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚    Embedding Client (50MB)             â”‚ Ã— 10 clients
â”‚ â€¢ Lightweight proxy                     â”‚
â”‚ â€¢ HTTP to ML-Base                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
                     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚      ML-Base Service (1.5GB)           â”‚ Ã— 1 service
â”‚ â€¢ Centralized inference                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```
**Improvement**: 50MB Ã— 10 + 1.5GB = 2.0GB (65% reduction)

### v3.0 (Production Scaling)  
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚    Embedding Client v3 (512MB with L1 cache)                  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                â”‚
â”‚  Phase 1: Multi-Tier Sutra-Native Cache                       â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚ L1: In-Memory LRU (10K entries, 68% hit, ~2Âµs)          â”‚ â”‚
â”‚  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤ â”‚
â”‚  â”‚ L2: Sutra Storage (100K concepts, 17% hit, ~2ms)        â”‚ â”‚
â”‚  â”‚     â€¢ TCP to storage-cache-shard:50052                  â”‚ â”‚
â”‚  â”‚     â€¢ HNSW vector indexing                              â”‚ â”‚
â”‚  â”‚     â€¢ WAL-backed persistence                            â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                                                                â”‚
â”‚  Cache Miss Fallback: L1 â†’ L2 â†’ ML-Base (15% miss)            â”‚
â”‚                                                                â”‚
â”‚  Endpoints:                                                    â”‚
â”‚  â€¢ POST /embed - Generate embeddings                          â”‚
â”‚  â€¢ GET /cache/stats - Cache performance metrics               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â”‚
                            â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚        Phase 2: HAProxy Load Balancer (leastconn)             â”‚
â”‚                     Port: 8887                                 â”‚
â”‚                                                                â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”‚
â”‚  â”‚ ML-Base-1    â”‚  â”‚ ML-Base-2    â”‚  â”‚ ML-Base-3    â”‚        â”‚
â”‚  â”‚ 6GB          â”‚  â”‚ 6GB          â”‚  â”‚ 6GB          â”‚        â”‚
â”‚  â”‚ 256-dim      â”‚  â”‚ 256-dim      â”‚  â”‚ 256-dim      â”‚        â”‚
â”‚  â”‚ 667ms/req    â”‚  â”‚ 667ms/req    â”‚  â”‚ 667ms/req    â”‚        â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â”‚
â”‚                                                                â”‚
â”‚  Phase 0: Matryoshka Truncation (768 â†’ 256/512/768 dim)       â”‚
â”‚  â€¢ Layer normalization before truncation                      â”‚
â”‚  â€¢ 3Ã— faster inference (2000ms â†’ 667ms)                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚      Sutra Storage Cache Shard (Phase 1: L2)                  â”‚
â”‚              Zero Redis | 100% Sutra-Native                    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  â€¢ Port: 50052 (dedicated cache shard)                        â”‚
â”‚  â€¢ Vector Dimension: 256 (matches Phase 0)                     â”‚
â”‚  â€¢ LRU Eviction: 100K concepts max                             â”‚
â”‚  â€¢ TTL: 24 hours (configurable)                                â”‚
â”‚  â€¢ WAL-backed persistence (survives restarts)                  â”‚
â”‚  â€¢ HNSW vector indexing (~2ms lookups)                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Performance Comparison:**
| Version | Throughput       | Latency     | Cache | User Capacity |
|---------|-----------------|-------------|-------|---------------|
| v1.x    | N/A             | ~2000ms     | None  | 50-100        |
| v2.0    | 0.14 concepts/s | ~2000ms     | None  | 100-200       |
| v3.0    | 8.8 concepts/s  | 50ms (hit)  | 85%   | 1,500-3,000   |

**v3.0 Benefits:**
- **21Ã— Performance**: 0.14 â†’ 8.8 concepts/sec throughput
- **85% Cache Hit Rate**: L1 (68%) + L2 (17%) combined
- **Zero External Dependencies**: 100% Sutra-native (no Redis)
- **Cost Effective**: 18Ã— cheaper per concept at scale

---

## ðŸš€ Quick Start

### 1. Deploy Architecture
```bash
# Deploy with ML-Base service
SUTRA_EDITION=simple ./sutra deploy

# Verify ML-Base service is running
curl http://localhost:8887/health

# Verify embedding client
curl http://localhost:8888/health
```

### 2. Generate Embeddings  
```bash
# Same API as v1.x - no changes needed!
curl -X POST http://localhost:8888/embed \
  -H "Content-Type: application/json" \
  -d '{
    "texts": ["Artificial intelligence is transforming healthcare"],
    "normalize": true
  }' | jq

# Response (same format as v1.x)
{
  "embeddings": [[0.1, -0.2, 0.3, ...]],
  "dimension": 768,
  "model": "nomic-ai/nomic-embed-text-v1.5", 
  "processing_time_ms": 23.4,
  "cached": false
}
```

---

## ðŸ”§ API Reference

### Client Endpoints (Port 8888)

All endpoints maintain v1.x compatibility while adding new features:

#### Generate Embeddings
```http
POST /embed
```

**Request** (unchanged from v1.x):
```json
{
  "texts": ["Text to embed", "Another text"],
  "normalize": true
}
```

**Response** (enhanced with cache info):
```json
{
  "embeddings": [
    [0.1, -0.2, 0.3, ...],
    [0.4, 0.1, -0.5, ...]
  ],
  "dimension": 768,
  "model": "nomic-ai/nomic-embed-text-v1.5",
  "processing_time_ms": 34.7,
  "cached": true,        // NEW: Indicates cache hit
  "cache_hits": 1,       // NEW: Number of cache hits in batch
  "ml_base_time_ms": 0   // NEW: Time spent in ML-Base (0 if cached)
}
```

#### Health Check
```http
GET /health
```

**Response** (enhanced with ML-Base status):
```json
{
  "healthy": true,
  "service": "embedding-client-v2",
  "version": "2.0.0",
  "ml_base_healthy": true,     // NEW: ML-Base service status
  "cache_enabled": true,       // NEW: Local cache status
  "api_compatible": "v1.x",    // NEW: Compatibility indicator
  "memory_usage_mb": 45        // NEW: Lightweight memory usage
}
```

#### Client Statistics  
```http
GET /stats
```

**Response** (new endpoint):
```json
{
  "service": "embedding-client-v2",
  "requests_served": 1500,
  "cache_stats": {
    "hits": 1200,
    "misses": 300, 
    "hit_rate": 0.80,
    "memory_used_mb": 12
  },
  "ml_base_stats": {
    "requests_forwarded": 300,
    "avg_response_time_ms": 45.2,
    "error_rate": 0.001
  },
  "uptime_seconds": 3600
}
```

---

## ðŸ—ï¸ Integration with ML-Base Service

### Request Flow

1. **Client Request**: POST /embed to embedding client (port 8888)
2. **Cache Check**: Local TTL cache lookup (1-2ms if hit)
3. **ML-Base Proxy**: Forward to ML-Base service (port 8887) if cache miss
4. **Response Caching**: Cache ML-Base response locally with TTL
5. **Client Response**: Return embeddings with cache metadata

### Configuration

**Environment Variables**:
```bash
# ML-Base Service Integration
ML_BASE_SERVICE_URL=http://ml-base-service:8887
ML_BASE_TIMEOUT=30
ML_BASE_MAX_RETRIES=3

# Local Caching  
EMBEDDING_CACHE_SIZE=1000     # Max cache entries
EMBEDDING_CACHE_TTL=3600      # Cache TTL in seconds

# Edition Limits
SUTRA_EDITION=simple          # simple|community|enterprise
```

**Docker Compose Integration**:
```yaml
embedding-single:
  image: sutra-embedding-service-v2:latest
  ports:
    - "8888:8888"
  environment:
    - ML_BASE_SERVICE_URL=http://ml-base-service:8887
    - SUTRA_EDITION=${SUTRA_EDITION:-simple}
  depends_on:
    - ml-base-service
  deploy:
    resources:
      limits:
        memory: 256M      # 92% reduction from 2GB
      reservations:
        memory: 128M
```

---

## ðŸ“Š Performance Characteristics

### Latency Breakdown

| Scenario | v1.x (Monolithic) | v2.0 (Client) | Improvement |
|----------|-------------------|---------------|-------------|
| **Cache Hit** | N/A | 1-2ms | **New capability** |
| **Cache Miss** | 20-50ms | 25-55ms | ~5ms overhead |
| **Cold Start** | 5-15s | 3-5s | **65% faster** |

### Resource Utilization

| Metric | v1.x (Per Instance) | v2.0 (Per Client) | Improvement |
|--------|---------------------|-------------------|-------------|
| **Memory** | 1.38GB | 128MB | **92% reduction** |
| **Storage** | 1.38GB | 50MB | **96% reduction** |
| **CPU** | High (inference) | Low (proxy only) | **Minimal usage** |
| **Startup** | 15-30s | 5-10s | **60% faster** |

### Scaling Comparison

**v1.x Scaling (Monolithic)**:
```
3 instances = 3 Ã— 1.38GB = 4.14GB total
Max capacity: ~150 req/s total
```

**v2.0 Scaling (ML-Base Client)**:
```
10 clients + 1 ML-Base = 10 Ã— 50MB + 1.5GB = 2.0GB total  
Max capacity: ~500 req/s total (ML-Base handles batching)
```

---

## ðŸ”§ Configuration & Deployment

### Edition-Aware Limits

| Edition | Cache Size | Cache TTL | Concurrent Requests |
|---------|------------|-----------|-------------------|
| **Simple** | 500 entries | 1 hour | 5 per client |
| **Community** | 1000 entries | 1 hour | 20 per client |
| **Enterprise** | 2000 entries | 2 hours | 100 per client |

### Docker Configuration

**Lightweight Dockerfile** (`Dockerfile.v2`):
```dockerfile
FROM python:3.11-slim

# Install minimal dependencies
COPY requirements-v2.txt requirements.txt
RUN pip install --no-cache-dir -r requirements.txt

# Copy lightweight client code
COPY main_v2.py main.py
COPY ../sutra-ml-base-service/client.py ./
COPY ../sutra-ml-base-service/config.py ./
COPY ../sutra-ml-base-service/monitoring.py ./

# Minimal resource usage
ENV PYTHONUNBUFFERED=1
ENV EMBEDDING_PORT=8888

EXPOSE 8888
CMD ["python", "-m", "uvicorn", "main:app", "--host", "0.0.0.0", "--port", "8888"]
```

**Lightweight Requirements** (`requirements-v2.txt`):
```
fastapi>=0.104.0
uvicorn[standard]>=0.24.0
httpx>=0.25.0
pydantic>=2.0.0
prometheus-client>=0.19.0
structlog>=23.0.0
tenacity>=8.0.0
psutil>=5.9.0
```

---

## ðŸ› Troubleshooting

### Common Issues

#### ML-Base Service Not Available

**Symptoms**:
```json
{
  "error": "ML-Base service unavailable",
  "ml_base_healthy": false
}
```

**Solutions**:
```bash
# Check ML-Base service health
curl http://localhost:8887/health

# Restart ML-Base service if needed
docker restart sutra-ml-base

# Check network connectivity
docker exec embedding-single curl http://ml-base-service:8887/health
```

#### Cache Not Working

**Symptoms**: `"cached": false` in all responses

**Solutions**:
```bash
# Check cache configuration
curl http://localhost:8888/stats | jq '.cache_stats'

# Verify cache is enabled
docker logs embedding-single | grep -i cache

# Clear and restart cache
curl -X DELETE http://localhost:8888/cache
```

#### High Latency

**Symptoms**: `processing_time_ms` > 100ms consistently

**Solutions**:
```bash
# Check ML-Base service performance
curl http://localhost:8887/health | jq '.active_requests'

# Monitor cache hit rate (should be >80%)
curl http://localhost:8888/stats | jq '.cache_stats.hit_rate'

# Scale ML-Base service if needed
docker-compose up -d --scale ml-base-service=2
```

### Debug Mode

```bash
# Enable debug logging
LOG_LEVEL=DEBUG docker-compose up -d embedding-single

# Monitor request flow
docker logs embedding-single --follow | grep -E "(cache|ml-base|embed)"

# Test cache behavior
curl -X POST http://localhost:8888/embed \
  -d '{"texts": ["test cache"]}' | jq '.cached'
# First request: false, second request: true
```

---

## ðŸš€ Migration Guide

### From v1.x to v2.0

**1. Update Docker Compose**:
```yaml
# OLD (v1.x)
sutra-embedding-service:
  image: sutra-embedding-service:latest
  deploy:
    resources:
      limits:
        memory: 2G

# NEW (v2.0)  
embedding-single:
  image: sutra-embedding-service-v2:latest
  environment:
    - ML_BASE_SERVICE_URL=http://ml-base-service:8887
  depends_on:
    - ml-base-service
  deploy:
    resources:
      limits:
        memory: 256M
```

**2. No Client Code Changes**:
```python
# This code continues to work unchanged!
import httpx

async with httpx.AsyncClient() as client:
    response = await client.post(
        "http://localhost:8888/embed",
        json={"texts": ["Hello world"], "normalize": True}
    )
    embeddings = response.json()["embeddings"]
```

**3. Enhanced Monitoring**:
```bash
# NEW: Cache statistics
curl http://localhost:8888/stats

# NEW: ML-Base integration status
curl http://localhost:8888/health | jq '.ml_base_healthy'
```

**4. Resource Optimization**:
- Reduce memory limits from 2GB to 256MB
- Scale horizontally without storage penalty
- Monitor ML-Base service as new central dependency

---

## ðŸ”— Related Documentation

- **[ML-Base Service Architecture](../ml-foundation/ML_BASE_SERVICE.md)** - Central inference platform
- **[Main System Architecture](../ARCHITECTURE.md)** - Updated system overview  
- **[Deployment Guide](../deployment/)** - ML-Base service deployment
- **[Performance Benchmarks](../performance/)** - v2.0 performance analysis

---

**Built on ML-Base Service Architecture v2.0.0**  
**Status**: âœ… Production-Ready  
**API Compatibility**: Full backward compatibility with v1.x  
**Last Updated**: October 28, 2025