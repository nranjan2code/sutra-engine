# Embedding Service Scaling - Quick Start Guide

## TL;DR - Get 10x Performance in 1 Week

This guide provides **copy-paste implementations** for the most impactful optimizations from the [full scaling strategy](./EMBEDDING_SCALING_STRATEGY.md).

---

## Phase 1: Redis Caching (Day 1-2) ‚Üí 5x improvement

### Step 1: Add Redis to Docker Compose

```yaml
# Add to .sutra/compose/production.yml

services:
  # Redis Cache for Embeddings
  redis-cache:
    image: redis:7-alpine
    container_name: sutra-works-redis-cache
    command: redis-server --maxmemory 500mb --maxmemory-policy allkeys-lru
    expose:
      - "6379"
    volumes:
      - redis-cache-data:/data
    networks:
      - sutra-network
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 10s
      timeout: 3s
      retries: 3

# Add to volumes section
volumes:
  redis-cache-data:
    driver: local
```

### Step 2: Update Embedding Service Configuration

```yaml
# Modify embedding-single service in production.yml

embedding-single:
  environment:
    # Existing config...
    - EMBEDDING_CACHE_SIZE=50000        # Increased from 1000
    - EMBEDDING_CACHE_TTL=86400         # 24 hours
    - REDIS_ENABLED=true
    - REDIS_HOST=redis-cache
    - REDIS_PORT=6379
    - REDIS_CACHE_TTL=604800            # 7 days
  depends_on:
    ml-base-service:
      condition: service_healthy
    redis-cache:
      condition: service_healthy
```

### Step 3: Install Redis Client

```bash
# Add to packages/sutra-embedding-service/requirements.txt
redis==5.0.1
```

### Step 4: Implement Multi-Tier Cache

Create `packages/sutra-embedding-service/cache_manager.py`:

```python
#!/usr/bin/env python3
"""Multi-tier caching for embedding service"""

import hashlib
import logging
import pickle
from typing import Optional, List
from collections import OrderedDict

try:
    import redis
    HAS_REDIS = True
except ImportError:
    HAS_REDIS = False

logger = logging.getLogger(__name__)

class LRUCache:
    """Simple LRU cache implementation"""
    
    def __init__(self, max_size: int = 50000):
        self.cache = OrderedDict()
        self.max_size = max_size
        self.hits = 0
        self.misses = 0
    
    def get(self, key: str) -> Optional[List[float]]:
        if key in self.cache:
            self.cache.move_to_end(key)
            self.hits += 1
            return self.cache[key]
        self.misses += 1
        return None
    
    def set(self, key: str, value: List[float]):
        if key in self.cache:
            self.cache.move_to_end(key)
        else:
            self.cache[key] = value
            if len(self.cache) > self.max_size:
                self.cache.popitem(last=False)
    
    def clear(self):
        self.cache.clear()
        self.hits = 0
        self.misses = 0
    
    def stats(self):
        total = self.hits + self.misses
        hit_rate = self.hits / total if total > 0 else 0
        return {
            "size": len(self.cache),
            "hits": self.hits,
            "misses": self.misses,
            "hit_rate": hit_rate
        }

class MultiTierCache:
    """
    Multi-tier cache: L1 (memory) ‚Üí L2 (Redis) ‚Üí Miss
    
    Provides 70-90% cache hit rate for embedding requests
    """
    
    def __init__(self, 
                 l1_max_size: int = 50000,
                 redis_host: str = "redis-cache",
                 redis_port: int = 6379,
                 redis_enabled: bool = True):
        
        # L1: In-memory LRU cache (fastest)
        self.l1 = LRUCache(max_size=l1_max_size)
        
        # L2: Redis distributed cache (fast)
        self.redis_enabled = redis_enabled and HAS_REDIS
        if self.redis_enabled:
            try:
                self.redis = redis.Redis(
                    host=redis_host,
                    port=redis_port,
                    decode_responses=False,
                    socket_connect_timeout=1,
                    socket_timeout=1
                )
                # Test connection
                self.redis.ping()
                logger.info(f"Connected to Redis at {redis_host}:{redis_port}")
            except Exception as e:
                logger.warning(f"Redis connection failed: {e}. Falling back to L1 only.")
                self.redis_enabled = False
                self.redis = None
        else:
            self.redis = None
            logger.info("Redis cache disabled")
        
        # Statistics
        self.l2_hits = 0
        self.l2_misses = 0
    
    def get(self, text: str, model: str = "default") -> Optional[List[float]]:
        """Get cached embedding or None"""
        cache_key = self._cache_key(text, model)
        
        # L1 check (fastest - microseconds)
        result = self.l1.get(cache_key)
        if result is not None:
            return result
        
        # L2 check (fast - milliseconds)
        if self.redis_enabled:
            try:
                result_bytes = self.redis.get(cache_key)
                if result_bytes:
                    result = pickle.loads(result_bytes)
                    # Promote to L1
                    self.l1.set(cache_key, result)
                    self.l2_hits += 1
                    return result
                else:
                    self.l2_misses += 1
            except Exception as e:
                logger.warning(f"Redis get error: {e}")
                self.l2_misses += 1
        
        return None  # Complete cache miss
    
    def set(self, text: str, embedding: List[float], model: str = "default", ttl: int = 86400):
        """Store embedding in cache"""
        cache_key = self._cache_key(text, model)
        
        # Store in L1 (always)
        self.l1.set(cache_key, embedding)
        
        # Store in L2 (with TTL)
        if self.redis_enabled:
            try:
                self.redis.setex(
                    cache_key,
                    ttl,
                    pickle.dumps(embedding)
                )
            except Exception as e:
                logger.warning(f"Redis set error: {e}")
    
    def clear(self):
        """Clear all caches"""
        self.l1.clear()
        if self.redis_enabled:
            try:
                # Only clear our namespace
                pattern = "emb:*"
                cursor = 0
                while True:
                    cursor, keys = self.redis.scan(cursor, match=pattern, count=1000)
                    if keys:
                        self.redis.delete(*keys)
                    if cursor == 0:
                        break
            except Exception as e:
                logger.error(f"Redis clear error: {e}")
        
        self.l2_hits = 0
        self.l2_misses = 0
    
    def stats(self):
        """Get cache statistics"""
        l1_stats = self.l1.stats()
        
        total_l2 = self.l2_hits + self.l2_misses
        l2_hit_rate = self.l2_hits / total_l2 if total_l2 > 0 else 0
        
        total_requests = l1_stats["hits"] + l1_stats["misses"]
        total_hits = l1_stats["hits"] + self.l2_hits
        total_hit_rate = total_hits / total_requests if total_requests > 0 else 0
        
        return {
            "l1": l1_stats,
            "l2": {
                "hits": self.l2_hits,
                "misses": self.l2_misses,
                "hit_rate": l2_hit_rate,
                "enabled": self.redis_enabled
            },
            "total": {
                "requests": total_requests,
                "hits": total_hits,
                "misses": self.l2_misses,
                "hit_rate": total_hit_rate
            }
        }
    
    def _cache_key(self, text: str, model: str) -> str:
        """Generate cache key from text and model"""
        text_hash = hashlib.sha256(text.encode('utf-8')).hexdigest()[:16]
        return f"emb:{model}:{text_hash}"
```

### Step 5: Integrate into Embedding Service

Update `packages/sutra-embedding-service/main.py`:

```python
# Add imports at top
import os
from cache_manager import MultiTierCache

# In EmbeddingService class __init__:
class EmbeddingService:
    def __init__(self):
        # ... existing code ...
        
        # Initialize multi-tier cache
        self.cache = MultiTierCache(
            l1_max_size=int(os.getenv("EMBEDDING_CACHE_SIZE", "50000")),
            redis_host=os.getenv("REDIS_HOST", "redis-cache"),
            redis_port=int(os.getenv("REDIS_PORT", "6379")),
            redis_enabled=os.getenv("REDIS_ENABLED", "true").lower() == "true"
        )
        
        logger.info("Multi-tier cache initialized")

# In generate_embeddings method:
async def generate_embeddings(self, request: EmbeddingRequest) -> EmbeddingResponse:
    start_time = time.time()
    
    # Check cache for each text
    cached_embeddings = []
    texts_to_compute = []
    
    for text in request.texts:
        cached = self.cache.get(text, request.model_id or "default")
        if cached is not None:
            cached_embeddings.append((len(texts_to_compute), cached))
        else:
            texts_to_compute.append(text)
    
    # Generate embeddings for cache misses only
    new_embeddings = []
    if texts_to_compute:
        # Call ML-Base service
        response = await self.ml_base_client.embed(
            model_id=request.model_id or "default",
            texts=texts_to_compute,
            normalize=request.normalize
        )
        new_embeddings = response["embeddings"]
        
        # Cache new embeddings
        for text, embedding in zip(texts_to_compute, new_embeddings):
            self.cache.set(
                text, 
                embedding, 
                request.model_id or "default",
                request.cache_ttl_seconds or 86400
            )
    
    # Combine cached and new embeddings in correct order
    all_embeddings = []
    new_idx = 0
    for i, text in enumerate(request.texts):
        # Check if this position had a cached result
        cached_result = next((emb for pos, emb in cached_embeddings if pos == new_idx), None)
        if cached_result is not None:
            all_embeddings.append(cached_result)
        else:
            all_embeddings.append(new_embeddings[new_idx])
            new_idx += 1
    
    processing_time = (time.time() - start_time) * 1000
    
    return EmbeddingResponse(
        embeddings=all_embeddings,
        dimension=len(all_embeddings[0]),
        model=request.model_id or "default",
        processing_time_ms=processing_time,
        cached_count=len(cached_embeddings),
        edition=SUTRA_EDITION,
        batch_size=len(request.texts)
    )

# Add cache stats endpoint:
@app.get("/cache/stats")
async def get_cache_stats():
    """Get cache statistics"""
    return self.cache.stats()
```

### Step 6: Deploy and Test

```bash
# Rebuild embedding service
SUTRA_EDITION=simple ./sutra build embedding-service

# Deploy with Redis
SUTRA_EDITION=simple ./sutra deploy

# Test cache performance
curl http://localhost:8080/api/health
curl http://localhost:8888/cache/stats
```

**Expected Results:**
- First query: ~2000ms (cache miss)
- Second identical query: ~5ms (cache hit)
- Overall hit rate: 60-80% after warmup

---

## Phase 2: ML-Base Replicas (Day 3-5) ‚Üí 10x improvement

### Step 1: Create HAProxy Configuration

Create `haproxy/ml-base-lb.cfg`:

```
global
    maxconn 4096
    log stdout format raw local0

defaults
    mode http
    timeout connect 5s
    timeout client 60s
    timeout server 60s
    log global
    option httplog

frontend ml_base_frontend
    bind *:8887
    default_backend ml_base_backend

backend ml_base_backend
    balance leastconn
    option httpchk GET /health
    http-check expect status 200
    
    server ml-base-1 ml-base-1:8887 check inter 10s fall 3 rise 2 weight 100
    server ml-base-2 ml-base-2:8887 check inter 10s fall 3 rise 2 weight 100
    server ml-base-3 ml-base-3:8887 check inter 10s fall 3 rise 2 weight 100
```

### Step 2: Add to Docker Compose

```yaml
# Add to .sutra/compose/production.yml

services:
  # ML-Base Load Balancer
  ml-base-lb:
    image: haproxy:2.9-alpine
    container_name: sutra-works-ml-base-lb
    expose:
      - "8887"
    volumes:
      - ../../haproxy/ml-base-lb.cfg:/usr/local/etc/haproxy/haproxy.cfg:ro
    networks:
      - sutra-network
    depends_on:
      - ml-base-1
      - ml-base-2
      - ml-base-3
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "haproxy", "-c", "-f", "/usr/local/etc/haproxy/haproxy.cfg"]
      interval: 30s
      timeout: 5s
      retries: 3

  # ML-Base Replica 1
  ml-base-1:
    build:
      context: ../..
      dockerfile: ./packages/sutra-ml-base-service/Dockerfile
    image: sutra-works-ml-base-service:${SUTRA_VERSION:-latest}
    container_name: sutra-works-ml-base-1
    expose:
      - "8887"
    environment:
      - PYTHONUNBUFFERED=1
      - ML_BASE_PORT=8887
      - ML_BASE_HOST=0.0.0.0
      - INSTANCE_ID=ml-base-1
      - SUTRA_EDITION=${SUTRA_EDITION:-simple}
      - ML_BASE_MAX_BATCH_SIZE=64
      - ML_BASE_BATCH_TIMEOUT_MS=50
      - ML_BASE_MODEL_UNLOAD_TIMEOUT=300
    volumes:
      - ml-models-cache:/models/cache
    networks:
      - sutra-network
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8887/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s
    deploy:
      resources:
        limits:
          memory: 6G
        reservations:
          memory: 4G

  # ML-Base Replica 2
  ml-base-2:
    extends:
      service: ml-base-1
    container_name: sutra-works-ml-base-2
    environment:
      - PYTHONUNBUFFERED=1
      - ML_BASE_PORT=8887
      - ML_BASE_HOST=0.0.0.0
      - INSTANCE_ID=ml-base-2
      - SUTRA_EDITION=${SUTRA_EDITION:-simple}
      - ML_BASE_MAX_BATCH_SIZE=64
      - ML_BASE_BATCH_TIMEOUT_MS=50

  # ML-Base Replica 3
  ml-base-3:
    extends:
      service: ml-base-1
    container_name: sutra-works-ml-base-3
    environment:
      - PYTHONUNBUFFERED=1
      - ML_BASE_PORT=8887
      - ML_BASE_HOST=0.0.0.0
      - INSTANCE_ID=ml-base-3
      - SUTRA_EDITION=${SUTRA_EDITION:-simple}
      - ML_BASE_MAX_BATCH_SIZE=64
      - ML_BASE_BATCH_TIMEOUT_MS=50

# Update existing ml-base-service to use load balancer
embedding-single:
  environment:
    - ML_BASE_URL=http://ml-base-lb:8887  # Changed from ml-base-service
```

### Step 3: Update Storage Server

```yaml
# In storage-server service
storage-server:
  environment:
    # Change embedding service URL to use load balancer
    - SUTRA_EMBEDDING_SERVICE_URL=http://ml-base-lb:8887
```

### Step 4: Deploy

```bash
# Build services
SUTRA_EDITION=simple ./sutra build

# Deploy with replicas
SUTRA_EDITION=simple ./sutra deploy

# Verify all replicas are healthy
docker ps | grep ml-base
# Should show: ml-base-lb, ml-base-1, ml-base-2, ml-base-3

# Check HAProxy stats
curl http://localhost:8887/health
```

**Expected Results:**
- 3x throughput increase
- Load distributed across replicas
- Automatic failover if one replica fails

---

## Performance Validation

### Test Script

Create `scripts/validate_scaling.py`:

```python
#!/usr/bin/env python3
"""Validate embedding service scaling improvements"""

import requests
import time
import statistics
from concurrent.futures import ThreadPoolExecutor, as_completed

API_URL = "http://localhost:8080/api"

def test_single_request():
    """Test single embedding request latency"""
    concept = {
        "content": "Apple Inc (AAPL) stock price $150 on 2025-11-08",
        "metadata": {"company": "AAPL", "type": "financial"}
    }
    
    start = time.time()
    response = requests.post(f"{API_URL}/learn", json=concept, timeout=30)
    latency = (time.time() - start) * 1000
    
    return {
        "success": response.status_code == 201,
        "latency_ms": latency
    }

def test_concurrent_requests(num_requests: int = 10):
    """Test concurrent request handling"""
    results = []
    
    with ThreadPoolExecutor(max_workers=10) as executor:
        futures = [executor.submit(test_single_request) for _ in range(num_requests)]
        
        for future in as_completed(futures):
            results.append(future.result())
    
    return results

def test_cache_effectiveness():
    """Test cache hit rate with repeated queries"""
    concept = {
        "content": "Test concept for cache validation",
        "metadata": {"type": "test"}
    }
    
    # First request (cache miss)
    start = time.time()
    requests.post(f"{API_URL}/learn", json=concept, timeout=30)
    first_latency = (time.time() - start) * 1000
    
    # Second request (should be cached)
    start = time.time()
    requests.post(f"{API_URL}/learn", json=concept, timeout=30)
    second_latency = (time.time() - start) * 1000
    
    # Get cache stats
    cache_stats = requests.get("http://localhost:8888/cache/stats").json()
    
    return {
        "first_request_ms": first_latency,
        "cached_request_ms": second_latency,
        "speedup": first_latency / second_latency,
        "cache_hit_rate": cache_stats["total"]["hit_rate"]
    }

def main():
    print("üî¨ EMBEDDING SERVICE SCALING VALIDATION")
    print("=" * 60)
    
    # Test 1: Single request latency
    print("\n1Ô∏è‚É£ Single Request Latency")
    result = test_single_request()
    print(f"   Latency: {result['latency_ms']:.2f}ms")
    print(f"   Target: <2000ms ‚úì" if result['latency_ms'] < 2000 else "   Target: <2000ms ‚úó")
    
    # Test 2: Concurrent throughput
    print("\n2Ô∏è‚É£ Concurrent Request Throughput")
    results = test_concurrent_requests(10)
    latencies = [r['latency_ms'] for r in results]
    success_rate = sum(1 for r in results if r['success']) / len(results)
    
    print(f"   Requests: {len(results)}")
    print(f"   Success Rate: {success_rate * 100:.1f}%")
    print(f"   Avg Latency: {statistics.mean(latencies):.2f}ms")
    print(f"   P95 Latency: {sorted(latencies)[int(len(latencies)*0.95)]:.2f}ms")
    
    # Test 3: Cache effectiveness
    print("\n3Ô∏è‚É£ Cache Effectiveness")
    cache_result = test_cache_effectiveness()
    print(f"   First Request: {cache_result['first_request_ms']:.2f}ms")
    print(f"   Cached Request: {cache_result['cached_request_ms']:.2f}ms")
    print(f"   Speedup: {cache_result['speedup']:.1f}x")
    print(f"   Cache Hit Rate: {cache_result['cache_hit_rate']*100:.1f}%")
    
    # Summary
    print("\n" + "=" * 60)
    if (result['latency_ms'] < 2000 and 
        success_rate > 0.95 and 
        cache_result['cache_hit_rate'] > 0.5):
        print("‚úÖ SCALING VALIDATION PASSED")
        print("   System is ready for increased load")
    else:
        print("‚ö†Ô∏è SOME TESTS FAILED")
        print("   Review results and optimize further")

if __name__ == "__main__":
    main()
```

### Run Validation

```bash
# Install dependencies
pip install requests

# Run validation
python3 scripts/validate_scaling.py
```

**Expected Output:**
```
üî¨ EMBEDDING SERVICE SCALING VALIDATION
============================================================

1Ô∏è‚É£ Single Request Latency
   Latency: 1850.32ms
   Target: <2000ms ‚úì

2Ô∏è‚É£ Concurrent Request Throughput
   Requests: 10
   Success Rate: 100.0%
   Avg Latency: 2103.45ms
   P95 Latency: 2456.78ms

3Ô∏è‚É£ Cache Effectiveness
   First Request: 1923.45ms
   Cached Request: 4.32ms
   Speedup: 445.3x
   Cache Hit Rate: 73.2%

============================================================
‚úÖ SCALING VALIDATION PASSED
   System is ready for increased load
```

---

## Monitoring Dashboard

### Quick Metrics Check

```bash
# Cache stats
curl http://localhost:8888/cache/stats | jq

# ML-Base health
curl http://localhost:8887/health | jq

# HAProxy stats
echo "show stat" | nc localhost 9999
```

### Expected Metrics After Optimization

```json
{
  "cache": {
    "total_hit_rate": 0.73,
    "l1_hit_rate": 0.68,
    "l2_hit_rate": 0.05,
    "total_requests": 15234
  },
  "throughput": {
    "concepts_per_second": 1.4,
    "improvement_factor": 10.0
  },
  "latency": {
    "p50_ms": 45,
    "p95_ms": 350,
    "p99_ms": 850
  }
}
```

---

## Troubleshooting

### Redis Connection Issues

```bash
# Check Redis is running
docker ps | grep redis

# Test connection
docker exec sutra-works-redis-cache redis-cli ping
# Should return: PONG

# Check logs
docker logs sutra-works-redis-cache
```

### HAProxy Not Routing

```bash
# Check HAProxy config
docker exec sutra-works-ml-base-lb cat /usr/local/etc/haproxy/haproxy.cfg

# Check backend health
docker exec sutra-works-ml-base-lb wget -O- http://ml-base-1:8887/health

# View HAProxy logs
docker logs sutra-works-ml-base-lb -f
```

### Low Cache Hit Rate

```bash
# Check cache stats
curl http://localhost:8888/cache/stats

# Clear cache and warmup
curl -X POST http://localhost:8888/cache/clear
python3 scripts/production_scale_financial.py --companies 3 --days 2
```

---

## Next Steps

Once Phase 1-2 are complete:

1. **Validate Performance**: Run load tests to confirm 10x improvement
2. **Monitor Production**: Watch metrics for 1 week
3. **Plan GPU Upgrade**: If further scaling needed, proceed to GPU phase
4. **Model Optimization**: Consider INT8 quantization for additional 2-3x

---

## Cost Summary

**Phase 1-2 Implementation:**
- Development Time: 1 week (1 engineer)
- Infrastructure Cost: +$100/month (Redis + 2 ML-Base replicas)
- Performance Gain: 10x (0.14 ‚Üí 1.4 concepts/sec)

**ROI**: Support 1,000 users with minimal cost increase

---

*Quick Start Version: 1.0*  
*For full details, see [EMBEDDING_SCALING_STRATEGY.md](./EMBEDDING_SCALING_STRATEGY.md)*
