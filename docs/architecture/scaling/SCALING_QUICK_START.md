# Embedding Service Scaling - Quick Start Guide

## üéØ Sutra Philosophy: Zero External Dependencies

**We use our own storage engine for EVERYTHING** - caching, monitoring, queuing, and infrastructure. No Redis, no PostgreSQL, no Prometheus. Just Sutra Storage with Grid events for self-monitoring.

**See:** [EMBEDDING_SCALING_SUTRA_NATIVE.md](./EMBEDDING_SCALING_SUTRA_NATIVE.md) for complete Sutra-native architecture.

---

## TL;DR - Get 21x Performance in 2 Weeks

This guide provides **copy-paste implementations** for the most impactful optimizations from the [full scaling strategy](./EMBEDDING_SCALING_STRATEGY.md).

**NEW (Nov 2025):** Start with **Phase 0: Matryoshka 256-dim** for instant 3√ó improvement before any infrastructure changes!

**Zero backward compatibility constraints:** With 0 users, you can change dimensions freely!

---

## Phase 0: Matryoshka 256-dim (Day 1 - 2 hours) ‚Üí 3√ó improvement ‚≠ê

**The easiest win: Change 3 config lines**

### Why This First?
- ‚úÖ Zero cost ($0)
- ‚úÖ 2 hours implementation
- ‚úÖ 3√ó faster embeddings (2000ms ‚Üí 667ms)
- ‚úÖ 67% storage savings
- ‚úÖ Only 2% quality loss (imperceptible)
- ‚úÖ No infrastructure changes needed

### Step 1: Update Embedding Service Configuration

```yaml
# .sutra/compose/production.yml

# Update ML-Base service to use Matryoshka truncation
ml-base-service:
  environment:
    # Existing config...
    - MODEL_NAME=nomic-ai/nomic-embed-text-v1.5
    - MATRYOSHKA_DIM=256  # NEW: Truncate to 256 dimensions
```

### Step 2: Update Storage Configuration

```yaml
# .sutra/compose/production.yml

# Update all storage services
storage-server:
  environment:
    - VECTOR_DIMENSION=256  # Changed from 768

grid-event-storage:
  environment:
    - VECTOR_DIMENSION=256  # Changed from 768

user-storage-server:
  environment:
    - VECTOR_DIMENSION=256  # Changed from 768
```

### Step 3: Implement Matryoshka Truncation

Add to `packages/sutra-ml-base-service/main.py`:

```python
import os
import torch
import torch.nn.functional as F

# Add at top of file
MATRYOSHKA_DIM = int(os.getenv("MATRYOSHKA_DIM", "768"))

class MLModelManager:
    def _embed_texts_sync(self, model_info: Dict, texts: List[str], normalize: bool) -> List[List[float]]:
        """Synchronous embedding generation with Matryoshka support"""
        model = model_info["model"]
        tokenizer = model_info["tokenizer"]
        
        # Tokenize
        max_length = min(512, self.edition_manager.get_sequence_length_limit())
        inputs = tokenizer(
            texts, 
            padding=True, 
            truncation=True, 
            return_tensors="pt",
            max_length=max_length
        )
        
        # Generate full embeddings
        with torch.no_grad():
            outputs = model(**inputs)
            
            if hasattr(outputs, 'last_hidden_state'):
                token_embeddings = outputs.last_hidden_state
            else:
                token_embeddings = outputs[0]
            
            attention_mask = inputs['attention_mask']
            input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()
            
            # Mean pooling
            embeddings = torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(
                input_mask_expanded.sum(1), min=1e-9
            )
            
            # Apply Matryoshka truncation if enabled
            if MATRYOSHKA_DIM < 768:
                # Layer normalization before truncation
                embeddings = F.layer_norm(embeddings, normalized_shape=(embeddings.shape[1],))
                # Truncate to desired dimensions
                embeddings = embeddings[:, :MATRYOSHKA_DIM]
            
            # L2 normalize
            if normalize:
                embeddings = F.normalize(embeddings, p=2, dim=1)
        
        return embeddings.cpu().numpy().tolist()
```

### Step 4: Deploy

```bash
# Rebuild services with new config
SUTRA_EDITION=simple ./sutra build

# Deploy (if 0 users, fresh start)
SUTRA_EDITION=simple ./sutra deploy

# Verify dimension
curl http://localhost:50051/health | jq '.vector_dimension'
# Should output: 256

# Test embedding generation
curl -X POST http://localhost:8888/embed \
  -H "Content-Type: application/json" \
  -d '{"texts": ["Test embedding"], "normalize": true}' | jq '.embeddings[0] | length'
# Should output: 256
```

### Step 5: Validate Performance

```bash
# Run smoke tests
./sutra test smoke

# Check speed improvement
time curl -X POST http://localhost:8080/api/learn \
  -d '{"content": "Test concept for performance check"}'

# Before: ~2000ms
# After:  ~667ms (3√ó faster!)
```

**Expected Results:**
- Embedding generation: 2000ms ‚Üí 667ms (3√ó faster)
- Storage per concept: 3KB ‚Üí 1KB (67% savings)
- MTEB quality: 62.28 ‚Üí 61.04 (2% loss, imperceptible)

**üéâ You've unlocked 3√ó performance with zero infrastructure cost. Phase 1 builds on this to reach 7√ó total.**

---

## Phase 1: Sutra-Native Caching (Day 2-3) ‚Üí 7√ó improvement total

**Builds on Phase 0 (256-dim Matryoshka):** Now that embeddings generate 3√ó faster (667ms), adding Sutra-native caching provides 70%+ hit rate for another 2.3√ó multiplier.

**Combined improvement:** 3√ó (dimensions) √ó 2.3√ó (cache) = **7√ó total throughput**

**Philosophy:** We use our own storage engine for caching - no Redis, no external dependencies.  
**Cost:** +$30/month for dedicated cache shard  
**Time:** 2-3 days implementation  
**Result:** 0.42 ‚Üí 2.94 concepts/sec (supports 500 users)

### Step 1: Add Sutra Cache Shard to Docker Compose

```yaml
# Add to .sutra/compose/production.yml

services:
  # Dedicated Sutra Storage Cache Shard
  storage-cache-shard:
    build:
      context: ../..
      dockerfile: ./packages/sutra-storage/Dockerfile
    image: sutra-storage:${SUTRA_VERSION:-latest}
    container_name: sutra-works-storage-cache
    expose:
      - "50052"  # Different port from main storage
    volumes:
      - storage-cache-data:/data
    environment:
      - RUST_LOG=info
      - STORAGE_PATH=/data
      - STORAGE_PORT=50052
      - VECTOR_DIMENSION=${VECTOR_DIMENSION:-256}  # Match Phase 0
      - MATRYOSHKA_DIM=${MATRYOSHKA_DIM:-256}
      
      # Cache-Optimized Settings
      - SUTRA_ROLE=cache_shard
      - CACHE_MODE=true
      - MAX_CONCEPTS=100000
      - EVICTION_POLICY=lru
      - DEFAULT_TTL_SECONDS=86400  # 24 hours
      - MMAP_ENABLED=true
    networks:
      - sutra-network
    restart: unless-stopped
    deploy:
      resources:
        limits:
          memory: 2G
        reservations:
          memory: 1G

# Add to volumes section
volumes:
  storage-cache-data:
    driver: local
```

### Step 2: Update Embedding Service Configuration

```yaml
# Modify embedding-single service in production.yml

embedding-single:
  environment:
    # Existing config...
    - EMBEDDING_CACHE_SIZE=50000        # L1 in-memory cache
    - EMBEDDING_CACHE_TTL=3600          # 1 hour (L1)
    
    # Sutra-Native L2 Cache
    - SUTRA_CACHE_ENABLED=true
    - SUTRA_CACHE_HOST=storage-cache-shard
    - SUTRA_CACHE_PORT=50052
    - SUTRA_CACHE_TTL=86400             # 24 hours (L2)
  depends_on:
    ml-base-service:
      condition: service_healthy
    storage-cache-shard:
      condition: service_healthy
```

### Step 3: Install Sutra TCP Client

```bash
# Already available in workspace
# packages/sutra-storage-client-tcp (Rust crate with Python bindings)
# No additional dependencies needed!
```

### Step 4: Implement Sutra-Native Multi-Tier Cache

Create `packages/sutra-embedding-service/sutra_cache_client.py`:

```python
#!/usr/bin/env python3
"""Sutra-native multi-tier caching for embedding service"""

import hashlib
import logging
import asyncio
from typing import Optional, List
from collections import OrderedDict
from datetime import datetime, timedelta

# Import our own TCP protocol client
from sutra_storage_client_tcp import StorageClient, ConceptMetadata

logger = logging.getLogger(__name__)

class LRUCache:
    """Simple LRU cache implementation (L1: in-memory)"""
    
    def __init__(self, max_size: int = 50000):
        self.cache = OrderedDict()
        self.max_size = max_size
        self.hits = 0
        self.misses = 0
    
    def get(self, key: str) -> Optional[List[float]]:
        if key in self.cache:
            self.cache.move_to_end(key)
            self.hits += 1
            return self.cache[key]
        self.misses += 1
        return None
    
    def set(self, key: str, value: List[float]):
        if key in self.cache:
            self.cache.move_to_end(key)
        else:
            self.cache[key] = value
            if len(self.cache) > self.max_size:
                self.cache.popitem(last=False)
    
    def clear(self):
        self.cache.clear()
        self.hits = 0
        self.misses = 0
    
    def stats(self):
        total = self.hits + self.misses
        hit_rate = self.hits / total if total > 0 else 0
        return {
            "size": len(self.cache),
            "hits": self.hits,
            "misses": self.misses,
            "hit_rate": hit_rate
        }

class SutraNativeCache:
    """
    Multi-tier cache: L1 (memory) ‚Üí L2 (Sutra Storage) ‚Üí Miss
    
    Provides 70-90% cache hit rate using our own storage engine
    Zero external dependencies (no Redis!)
    """
    
    def __init__(self, 
                 l1_max_size: int = 50000,
                 sutra_cache_host: str = "storage-cache-shard",
                 sutra_cache_port: int = 50052,
                 sutra_cache_enabled: bool = True):
        
        # L1: In-memory LRU cache (fastest)
        self.l1 = LRUCache(max_size=l1_max_size)
        
        # L2: Sutra Storage cache shard (fast + persistent)
        self.sutra_enabled = sutra_cache_enabled
        if self.sutra_enabled:
            try:
                self.sutra_client = StorageClient(
                    host=sutra_cache_host,
                    port=sutra_cache_port
                )
                # Test connection
                self.sutra_client.health_check()
                logger.info(f"Connected to Sutra cache at {sutra_cache_host}:{sutra_cache_port}")
            except Exception as e:
                logger.warning(f"Sutra cache connection failed: {e}. Falling back to L1 only.")
                self.sutra_enabled = False
                self.sutra_client = None
        else:
            self.sutra_client = None
            logger.info("Sutra cache disabled (L1 only)")
        
        # Statistics
        self.l2_hits = 0
        self.l2_misses = 0
    
    def get(self, text: str, model: str = "default") -> Optional[List[float]]:
        """Get cached embedding or None"""
        cache_key = self._cache_key(text, model)
        
        # L1 check (fastest - microseconds)
        result = self.l1.get(cache_key)
        if result is not None:
            return result
        
        # L2 check (Sutra Storage - fast, <1ms)
        if self.sutra_enabled:
            try:
                # Query Sutra cache shard by concept ID (cache_key)
                concept = self.sutra_client.get_concept_by_id(cache_key)
                if concept and concept.embedding:
                    result = concept.embedding
                    # Promote to L1
                    self.l1.set(cache_key, result)
                    self.l2_hits += 1
                    return result
                else:
                    self.l2_misses += 1
            except Exception as e:
                logger.warning(f"Sutra cache get error: {e}")
                self.l2_misses += 1
        
        return None  # Complete cache miss
    
    def set(self, text: str, embedding: List[float], model: str = "default", ttl: int = 86400):
        """Store embedding in cache"""
        cache_key = self._cache_key(text, model)
        
        # Store in L1 (always)
        self.l1.set(cache_key, embedding)
        
        # Store in L2 (Sutra Storage with TTL metadata)
        if self.sutra_enabled:
            try:
                # Store as concept with TTL metadata
                metadata = ConceptMetadata(
                    concept_type="embedding_cache",
                    created_at=datetime.now().isoformat(),
                    expires_at=(datetime.now() + timedelta(seconds=ttl)).isoformat(),
                    model=model
                )
                self.sutra_client.store_concept(
                    concept_id=cache_key,
                    content=text[:200],  # Store truncated text for debugging
                    embedding=embedding,
                    metadata=metadata
                )
            except Exception as e:
                logger.warning(f"Sutra cache set error: {e}")
    
    def clear(self):
        """Clear all caches"""
        self.l1.clear()
        if self.sutra_enabled:
            try:
                # Query all embedding_cache concepts and delete
                cache_concepts = self.sutra_client.query_by_type("embedding_cache")
                for concept in cache_concepts:
                    self.sutra_client.delete_concept(concept.id)
            except Exception as e:
                logger.error(f"Sutra cache clear error: {e}")
        
        self.l2_hits = 0
        self.l2_misses = 0
    
    def stats(self):
        """Get cache statistics"""
        l1_stats = self.l1.stats()
        
        total_l2 = self.l2_hits + self.l2_misses
        l2_hit_rate = self.l2_hits / total_l2 if total_l2 > 0 else 0
        
        total_requests = l1_stats["hits"] + l1_stats["misses"]
        total_hits = l1_stats["hits"] + self.l2_hits
        total_hit_rate = total_hits / total_requests if total_requests > 0 else 0
        
        return {
            "l1": l1_stats,
            "l2": {
                "hits": self.l2_hits,
                "misses": self.l2_misses,
                "hit_rate": l2_hit_rate,
                "enabled": self.sutra_enabled,
                "backend": "sutra-storage"
            },
            "total": {
                "requests": total_requests,
                "hits": total_hits,
                "misses": self.l2_misses,
                "hit_rate": total_hit_rate
            }
        }
    
    def _cache_key(self, text: str, model: str) -> str:
        """Generate cache key from text and model"""
        text_hash = hashlib.sha256(text.encode('utf-8')).hexdigest()[:16]
        return f"emb:{model}:{text_hash}"
```

### Step 5: Integrate into Embedding Service

Update `packages/sutra-embedding-service/main.py`:

```python
# Add imports at top
import os
from sutra_cache_client import SutraNativeCache

# In EmbeddingService class __init__:
class EmbeddingService:
    def __init__(self):
        # ... existing code ...
        
        # Initialize Sutra-native multi-tier cache
        self.cache = SutraNativeCache(
            l1_max_size=int(os.getenv("EMBEDDING_CACHE_SIZE", "50000")),
            sutra_cache_host=os.getenv("SUTRA_CACHE_HOST", "storage-cache-shard"),
            sutra_cache_port=int(os.getenv("SUTRA_CACHE_PORT", "50052")),
            sutra_cache_enabled=os.getenv("SUTRA_CACHE_ENABLED", "true").lower() == "true"
        )
        
        logger.info("Sutra-native multi-tier cache initialized")

# In generate_embeddings method:
async def generate_embeddings(self, request: EmbeddingRequest) -> EmbeddingResponse:
    start_time = time.time()
    
    # Check cache for each text
    cached_embeddings = []
    texts_to_compute = []
    
    for text in request.texts:
        cached = self.cache.get(text, request.model_id or "default")
        if cached is not None:
            cached_embeddings.append((len(texts_to_compute), cached))
        else:
            texts_to_compute.append(text)
    
    # Generate embeddings for cache misses only
    new_embeddings = []
    if texts_to_compute:
        # Call ML-Base service
        response = await self.ml_base_client.embed(
            model_id=request.model_id or "default",
            texts=texts_to_compute,
            normalize=request.normalize
        )
        new_embeddings = response["embeddings"]
        
        # Cache new embeddings
        for text, embedding in zip(texts_to_compute, new_embeddings):
            self.cache.set(
                text, 
                embedding, 
                request.model_id or "default",
                request.cache_ttl_seconds or 86400
            )
    
    # Combine cached and new embeddings in correct order
    all_embeddings = []
    new_idx = 0
    for i, text in enumerate(request.texts):
        # Check if this position had a cached result
        cached_result = next((emb for pos, emb in cached_embeddings if pos == new_idx), None)
        if cached_result is not None:
            all_embeddings.append(cached_result)
        else:
            all_embeddings.append(new_embeddings[new_idx])
            new_idx += 1
    
    processing_time = (time.time() - start_time) * 1000
    
    return EmbeddingResponse(
        embeddings=all_embeddings,
        dimension=len(all_embeddings[0]),
        model=request.model_id or "default",
        processing_time_ms=processing_time,
        cached_count=len(cached_embeddings),
        edition=SUTRA_EDITION,
        batch_size=len(request.texts)
    )

# Add cache stats endpoint:
@app.get("/cache/stats")
async def get_cache_stats():
    """Get cache statistics"""
    return self.cache.stats()
```

### Step 6: Deploy and Test

```bash
# Rebuild embedding service
SUTRA_EDITION=simple ./sutra build embedding-service

# Deploy with Sutra-native cache
SUTRA_EDITION=simple ./sutra deploy

# Verify cache shard is running
docker ps | grep storage-cache

# Test cache performance
curl http://localhost:8080/api/health
curl http://localhost:8888/cache/stats
```

**Expected Results:**
- First query: ~667ms (256-dim, cache miss)
- Second identical query: ~5ms (L1 cache hit)
- Third query after L1 eviction: ~20ms (L2 Sutra cache hit)
- Overall hit rate: 70-85% after warmup

**Why Sutra-native is better:**
- ‚úÖ WAL persistence: Cache survives restarts
- ‚úÖ Zero external dependencies: No Redis licensing/management
- ‚úÖ Semantic queries: Can reason about cached embeddings
- ‚úÖ Unified monitoring: Grid events track cache performance
- ‚úÖ Automatic TTL cleanup: Built into storage engine

---

## Phase 2: ML-Base Replicas (Day 3-5) ‚Üí 21√ó improvement total

**Builds on Phase 0+1 (Matryoshka + Cache):** With 2.94 concepts/sec baseline from Phase 1, adding 3 CPU replicas provides 3√ó horizontal scaling.

**Combined improvement:** 3√ó (dimensions) √ó 2.3√ó (cache) √ó 3√ó (replicas) = **21√ó total throughput**

**Cost:** +$420/month total ($140/replica √ó 3)  
**Time:** 3-5 days implementation  
**Result:** 2.94 ‚Üí 8.8 concepts/sec (supports 1,500 users without GPU)

**Key insight:** This is the **maximum CPU-only performance** before needing GPU acceleration.

### Step 1: Create HAProxy Configuration

Create `haproxy/ml-base-lb.cfg`:

```
global
    maxconn 4096
    log stdout format raw local0

defaults
    mode http
    timeout connect 5s
    timeout client 60s
    timeout server 60s
    log global
    option httplog

frontend ml_base_frontend
    bind *:8887
    default_backend ml_base_backend

backend ml_base_backend
    balance leastconn
    option httpchk GET /health
    http-check expect status 200
    
    server ml-base-1 ml-base-1:8887 check inter 10s fall 3 rise 2 weight 100
    server ml-base-2 ml-base-2:8887 check inter 10s fall 3 rise 2 weight 100
    server ml-base-3 ml-base-3:8887 check inter 10s fall 3 rise 2 weight 100
```

### Step 2: Add to Docker Compose

```yaml
# Add to .sutra/compose/production.yml

services:
  # ML-Base Load Balancer
  ml-base-lb:
    image: haproxy:2.9-alpine
    container_name: sutra-works-ml-base-lb
    expose:
      - "8887"
    volumes:
      - ../../haproxy/ml-base-lb.cfg:/usr/local/etc/haproxy/haproxy.cfg:ro
    networks:
      - sutra-network
    depends_on:
      - ml-base-1
      - ml-base-2
      - ml-base-3
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "haproxy", "-c", "-f", "/usr/local/etc/haproxy/haproxy.cfg"]
      interval: 30s
      timeout: 5s
      retries: 3

  # ML-Base Replica 1
  ml-base-1:
    build:
      context: ../..
      dockerfile: ./packages/sutra-ml-base-service/Dockerfile
    image: sutra-works-ml-base-service:${SUTRA_VERSION:-latest}
    container_name: sutra-works-ml-base-1
    expose:
      - "8887"
    environment:
      - PYTHONUNBUFFERED=1
      - ML_BASE_PORT=8887
      - ML_BASE_HOST=0.0.0.0
      - INSTANCE_ID=ml-base-1
      - SUTRA_EDITION=${SUTRA_EDITION:-simple}
      - ML_BASE_MAX_BATCH_SIZE=64
      - ML_BASE_BATCH_TIMEOUT_MS=50
      - ML_BASE_MODEL_UNLOAD_TIMEOUT=300
    volumes:
      - ml-models-cache:/models/cache
    networks:
      - sutra-network
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8887/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s
    deploy:
      resources:
        limits:
          memory: 6G
        reservations:
          memory: 4G

  # ML-Base Replica 2
  ml-base-2:
    extends:
      service: ml-base-1
    container_name: sutra-works-ml-base-2
    environment:
      - PYTHONUNBUFFERED=1
      - ML_BASE_PORT=8887
      - ML_BASE_HOST=0.0.0.0
      - INSTANCE_ID=ml-base-2
      - SUTRA_EDITION=${SUTRA_EDITION:-simple}
      - ML_BASE_MAX_BATCH_SIZE=64
      - ML_BASE_BATCH_TIMEOUT_MS=50

  # ML-Base Replica 3
  ml-base-3:
    extends:
      service: ml-base-1
    container_name: sutra-works-ml-base-3
    environment:
      - PYTHONUNBUFFERED=1
      - ML_BASE_PORT=8887
      - ML_BASE_HOST=0.0.0.0
      - INSTANCE_ID=ml-base-3
      - SUTRA_EDITION=${SUTRA_EDITION:-simple}
      - ML_BASE_MAX_BATCH_SIZE=64
      - ML_BASE_BATCH_TIMEOUT_MS=50

# Update existing ml-base-service to use load balancer
embedding-single:
  environment:
    - ML_BASE_URL=http://ml-base-lb:8887  # Changed from ml-base-service
```

### Step 3: Update Storage Server

```yaml
# In storage-server service
storage-server:
  environment:
    # Change embedding service URL to use load balancer
    - SUTRA_EMBEDDING_SERVICE_URL=http://ml-base-lb:8887
```

### Step 4: Deploy

```bash
# Build services
SUTRA_EDITION=simple ./sutra build

# Deploy with replicas
SUTRA_EDITION=simple ./sutra deploy

# Verify all replicas are healthy
docker ps | grep ml-base
# Should show: ml-base-lb, ml-base-1, ml-base-2, ml-base-3

# Check HAProxy stats
curl http://localhost:8887/health
```

**Expected Results:**
- 3x throughput increase
- Load distributed across replicas
- Automatic failover if one replica fails

---

## Performance Validation

### Test Script

Create `scripts/validate_scaling.py`:

```python
#!/usr/bin/env python3
"""Validate embedding service scaling improvements"""

import requests
import time
import statistics
from concurrent.futures import ThreadPoolExecutor, as_completed

API_URL = "http://localhost:8080/api"

def test_single_request():
    """Test single embedding request latency"""
    concept = {
        "content": "Apple Inc (AAPL) stock price $150 on 2025-11-08",
        "metadata": {"company": "AAPL", "type": "financial"}
    }
    
    start = time.time()
    response = requests.post(f"{API_URL}/learn", json=concept, timeout=30)
    latency = (time.time() - start) * 1000
    
    return {
        "success": response.status_code == 201,
        "latency_ms": latency
    }

def test_concurrent_requests(num_requests: int = 10):
    """Test concurrent request handling"""
    results = []
    
    with ThreadPoolExecutor(max_workers=10) as executor:
        futures = [executor.submit(test_single_request) for _ in range(num_requests)]
        
        for future in as_completed(futures):
            results.append(future.result())
    
    return results

def test_cache_effectiveness():
    """Test cache hit rate with repeated queries"""
    concept = {
        "content": "Test concept for cache validation",
        "metadata": {"type": "test"}
    }
    
    # First request (cache miss)
    start = time.time()
    requests.post(f"{API_URL}/learn", json=concept, timeout=30)
    first_latency = (time.time() - start) * 1000
    
    # Second request (should be cached)
    start = time.time()
    requests.post(f"{API_URL}/learn", json=concept, timeout=30)
    second_latency = (time.time() - start) * 1000
    
    # Get cache stats
    cache_stats = requests.get("http://localhost:8888/cache/stats").json()
    
    return {
        "first_request_ms": first_latency,
        "cached_request_ms": second_latency,
        "speedup": first_latency / second_latency,
        "cache_hit_rate": cache_stats["total"]["hit_rate"]
    }

def main():
    print("üî¨ EMBEDDING SERVICE SCALING VALIDATION")
    print("=" * 60)
    
    # Test 1: Single request latency
    print("\n1Ô∏è‚É£ Single Request Latency")
    result = test_single_request()
    print(f"   Latency: {result['latency_ms']:.2f}ms")
    print(f"   Target: <2000ms ‚úì" if result['latency_ms'] < 2000 else "   Target: <2000ms ‚úó")
    
    # Test 2: Concurrent throughput
    print("\n2Ô∏è‚É£ Concurrent Request Throughput")
    results = test_concurrent_requests(10)
    latencies = [r['latency_ms'] for r in results]
    success_rate = sum(1 for r in results if r['success']) / len(results)
    
    print(f"   Requests: {len(results)}")
    print(f"   Success Rate: {success_rate * 100:.1f}%")
    print(f"   Avg Latency: {statistics.mean(latencies):.2f}ms")
    print(f"   P95 Latency: {sorted(latencies)[int(len(latencies)*0.95)]:.2f}ms")
    
    # Test 3: Cache effectiveness
    print("\n3Ô∏è‚É£ Cache Effectiveness")
    cache_result = test_cache_effectiveness()
    print(f"   First Request: {cache_result['first_request_ms']:.2f}ms")
    print(f"   Cached Request: {cache_result['cached_request_ms']:.2f}ms")
    print(f"   Speedup: {cache_result['speedup']:.1f}x")
    print(f"   Cache Hit Rate: {cache_result['cache_hit_rate']*100:.1f}%")
    
    # Summary
    print("\n" + "=" * 60)
    if (result['latency_ms'] < 2000 and 
        success_rate > 0.95 and 
        cache_result['cache_hit_rate'] > 0.5):
        print("‚úÖ SCALING VALIDATION PASSED")
        print("   System is ready for increased load")
    else:
        print("‚ö†Ô∏è SOME TESTS FAILED")
        print("   Review results and optimize further")

if __name__ == "__main__":
    main()
```

### Run Validation

```bash
# Install dependencies
pip install requests

# Run validation
python3 scripts/validate_scaling.py
```

**Expected Output:**
```
üî¨ EMBEDDING SERVICE SCALING VALIDATION
============================================================

1Ô∏è‚É£ Single Request Latency
   Latency: 1850.32ms
   Target: <2000ms ‚úì

2Ô∏è‚É£ Concurrent Request Throughput
   Requests: 10
   Success Rate: 100.0%
   Avg Latency: 2103.45ms
   P95 Latency: 2456.78ms

3Ô∏è‚É£ Cache Effectiveness
   First Request: 1923.45ms
   Cached Request: 4.32ms
   Speedup: 445.3x
   Cache Hit Rate: 73.2%

============================================================
‚úÖ SCALING VALIDATION PASSED
   System is ready for increased load
```

---

## Monitoring Dashboard

### Quick Metrics Check

```bash
# Cache stats
curl http://localhost:8888/cache/stats | jq

# ML-Base health
curl http://localhost:8887/health | jq

# HAProxy stats
echo "show stat" | nc localhost 9999
```

### Expected Metrics After Optimization

```json
{
  "cache": {
    "total_hit_rate": 0.73,
    "l1_hit_rate": 0.68,
    "l2_hit_rate": 0.05,
    "total_requests": 15234
  },
  "throughput": {
    "concepts_per_second": 1.4,
    "improvement_factor": 10.0
  },
  "latency": {
    "p50_ms": 45,
    "p95_ms": 350,
    "p99_ms": 850
  }
}
```

---

## Troubleshooting

### Sutra Cache Connection Issues

```bash
# Check Sutra cache shard is running
docker ps | grep storage-cache

# Test connection via TCP
telnet storage-cache-shard 50052
# Should connect successfully

# Check logs
docker logs sutra-works-storage-cache

# Verify cache shard health
curl http://localhost:50052/health
```

### HAProxy Not Routing

```bash
# Check HAProxy config
docker exec sutra-works-ml-base-lb cat /usr/local/etc/haproxy/haproxy.cfg

# Check backend health
docker exec sutra-works-ml-base-lb wget -O- http://ml-base-1:8887/health

# View HAProxy logs
docker logs sutra-works-ml-base-lb -f
```

### Low Cache Hit Rate

```bash
# Check cache stats (L1 + L2 Sutra cache)
curl http://localhost:8888/cache/stats

# Query cache concepts via Sutra
"Show cache hit rate for last hour"
"Which concepts are cached most frequently?"

# Clear cache and warmup
curl -X POST http://localhost:8888/cache/clear
python3 scripts/production_scale_financial.py --companies 3 --days 2
```

---

## Next Steps

Once Phase 1-2 are complete:

1. **Validate Performance**: Run load tests to confirm 10x improvement
2. **Monitor Production**: Watch metrics for 1 week
3. **Plan GPU Upgrade**: If further scaling needed, proceed to GPU phase
4. **Model Optimization**: Consider INT8 quantization for additional 2-3x

---

## Cost Summary

**Phase 1-2 Implementation:**
- Development Time: 1 week (1 engineer)
- Infrastructure Cost: +$100/month (Sutra cache shard + 2 ML-Base replicas)
- Performance Gain: 10x (0.14 ‚Üí 1.4 concepts/sec)
- Zero External Dependencies: Pure Sutra architecture

**ROI**: Support 1,000 users with minimal cost increase + unified operations

---

*Quick Start Version: 1.0*  
*For full details, see [EMBEDDING_SCALING_STRATEGY.md](./EMBEDDING_SCALING_STRATEGY.md)*
