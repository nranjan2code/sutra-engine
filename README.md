# Sutra AI

An explainable AI system that learns in real-time without retraining. Every decision includes reasoning paths showing how it arrived at an answer.

## Why This Exists

Current AI systems (LLMs) are black boxes:
- You can't see how they make decisions
- You can't verify their reasoning
- You can't update them without complete retraining
- You can't use them in regulated industries that require explainability

We're building an alternative that:
- Shows its reasoning for every answer
- Learns incrementally from new information
- Provides audit trails for compliance
- Works without requiring GPUs or massive compute

## What It Does

Sutra AI combines graph-based reasoning with semantic embeddings:

1. **Graph reasoning**: Concepts connected by typed relationships (semantic, causal, temporal, hierarchical, compositional)
2. **Semantic embeddings**: Optional similarity matching to enhance reasoning
3. **Multi-strategy comparison**: Compare different reasoning approaches and see agreement scores
4. **Real-time learning**: Learn from new information without retraining
5. **Full audit trails**: Every decision logged with timestamps, confidence scores, and reasoning paths

## What Works (Proven End-to-End)

âœ… **Learn new knowledge** - Add concepts and relationships  
âœ… **Query with reasoning paths** - Get answers with explanations  
âœ… **Save to disk** - Persist knowledge (concepts, associations, embeddings)  
âœ… **Reload from disk** - Restore complete state after restart  
âœ… **Multi-strategy reasoning** - Compare graph-only vs semantic-enhanced  
âœ… **Audit trails** - Full compliance tracking  
âœ… **REST API** - Production-ready HTTP interface  

Tested with 5 concepts, ~100ms query latency, full persistence verified.

## Architecture

**12-Service Production Ecosystem** with TCP binary protocol and containerized deployment. All services communicate via high-performance TCP with a secure React-based control center for monitoring.

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        Docker Network (sutra-network)                         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  sutra-control â”‚    â”‚  sutra-client  â”‚    â”‚ sutra-markdown-web â”‚  â”‚
â”‚  â”‚  (React + Fast â”‚    â”‚   (Streamlit   â”‚    â”‚   (Markdown API)   â”‚  â”‚
â”‚  â”‚   API Gateway) â”‚    â”‚    UI Client)  â”‚    â”‚    Port: 8002     â”‚  â”‚
â”‚  â”‚   Port: 9000   â”‚    â”‚   Port: 8080   â”‚    â””â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                        â”‚
â”‚            â”‚                     â”‚            TCP                        â”‚
â”‚            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€ Binary                   â”‚
â”‚                                     â”‚  Protocol                      â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚   sutra-api     â”‚â—€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â–¶â”‚       storage-server         â”‚  â”‚
â”‚  â”‚   (FastAPI)     â”‚              â”‚  â”‚    (Rust TCP Server)        â”‚  â”‚
â”‚  â”‚   Port: 8000    â”‚              â”‚  â”‚      Port: 50051            â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â”‚  â””â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚            â”‚                     â”‚            â”‚                       â”‚
â”‚            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                                     â”‚            â”‚                       â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”‚  â”Œâ”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚ sutra-hybrid  â”‚â—€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”‚â—€â”´â”€â”€â”€â”€  sutra-ollama         â”‚  â”‚
â”‚  â”‚ (Embeddings + â”‚              â”‚  â”‚   (Local LLM Server)      â”‚  â”‚
â”‚  â”‚ Orchestration)â”‚              â”‚  â”‚      Port: 11434           â”‚  â”‚
â”‚  â”‚   Port: 8001   â”‚              â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â”‚                           â”‚
â”‚                                     â”‚                           â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”â—€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚
â”‚  â”‚      sutra-bulk-ingester       â”‚            ğŸ”¥ NEW SERVICE        â”‚
â”‚  â”‚   (High-Performance Rust)      â”‚            Port: 8005           â”‚
â”‚  â”‚      Port: 8005               â”‚         (Production Ready)      â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                   â”‚
â”‚                                                                    â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚                   Sutra Grid (Distributed Layer)                    â”‚  â”‚
â”‚  â”‚  Grid Master (7001 HTTP, 7002 TCP) â—€â”€â”€TCPâ”€â”€â–¶ Grid Agents (8001)        â”‚  â”‚
â”‚  â”‚  Event Storage (50052 TCP)                                         â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Core Services
- **sutra-control**: React-based monitoring center with Grid management and bulk ingester UI
- **sutra-client**: Streamlit web interface for interactive queries  
- **sutra-api**: Primary REST API for AI operations
- **sutra-hybrid**: Semantic embeddings and orchestration
- **storage-server**: Rust TCP core storage engine (57K writes/sec)
- **sutra-bulk-ingester**: ğŸ”¥ **NEW** High-performance Rust bulk data ingestion (1K-10K articles/min)
- **sutra-markdown-web**: Document processing API
- **sutra-ollama**: Local LLM inference server

All services communicate via gRPC internally, with REST APIs for external access. The control center provides secure monitoring without exposing internal implementation details.

### Sutra Grid - Distributed Storage Orchestration

**NEW**: Production-ready distributed infrastructure with complete Docker deployment and web UI integration.

Sutra Grid manages storage nodes across multiple agents with:
- **Bidirectional gRPC**: Master â†” Agent communication (ports 7001 HTTP, 7002 gRPC)
- **Event-Driven Monitoring**: 17 structured events â†’ knowledge graph (port 50052)
- **Auto-Recovery**: Crashed nodes restart automatically (up to 3 times)
- **Production Features**: Retry logic, timeouts, health monitoring, graceful degradation
- **Web UI**: Complete Grid management via Sutra Control Center (port 9000)

**Key Innovation**: Grid monitors itself using Sutra's own platform - proving event-driven observability works without external LMT (Logs/Metrics/Telemetry) stack.

**Status**: Production-Ready âœ…  
- Master: 11 events emitted
- Agent: 2 node lifecycle events  
- Storage: Events as queryable concepts
- Docker: Complete containerized deployment
- Control Center: Grid management UI integrated
- Testing: End-to-end verified

**Architecture Details**: See [docs/grid/architecture/GRID_ARCHITECTURE.md](docs/grid/architecture/GRID_ARCHITECTURE.md) and [DEPLOYMENT.md](DEPLOYMENT.md) for complete documentation.

## ğŸš¨ CRITICAL PRODUCTION REQUIREMENTS

**âš ï¸ Before deployment, you MUST read:**
- [`PRODUCTION_CHECKLIST.md`](PRODUCTION_CHECKLIST.md) - Mandatory pre-deployment verification
- [`docs/EMBEDDING_TROUBLESHOOTING.md`](docs/EMBEDDING_TROUBLESHOOTING.md) - Critical fixes applied

**The system will NOT function without:**
1. Ollama service with `granite-embedding:30m` model
2. Proper TCP protocol implementation
3. Environment variables correctly configured

## Quick Start

### 1. Deploy with Docker (Recommended)

**âš¡ Single command deployment:**

```bash
# First-time installation
./sutra-deploy.sh install

# Or start existing services
./sutra-deploy.sh up
```

**Access services:**
```bash
open http://localhost:9000    # Control Center (monitoring + Grid + bulk ingester)
open http://localhost:8080    # Interactive Client (queries)
open http://localhost:8000    # Primary API
```

**Manage deployment:**
```bash
./sutra-deploy.sh status      # Check system status
./sutra-deploy.sh logs        # View all logs
./sutra-deploy.sh maintenance # Interactive menu
./sutra-deploy.sh down        # Stop all services
```

**See [DEPLOYMENT.md](DEPLOYMENT.md) for complete documentation.**

### 2. Test End-to-End

```bash
# Run the end-to-end test
python test_direct_workflow.py
```

This tests: Learn â†’ Save â†’ Reload â†’ Query â†’ Multi-strategy â†’ Audit

### 2. Use the API

```bash
# Learn (thin proxy â†’ storage via gRPC)
curl -X POST http://localhost:8000/learn \
  -H "Content-Type: application/json" \
  -d '{"content": "Python is a programming language"}'

# Health
curl -s http://localhost:8000/health
```

## What We're Working Toward

**Short-term** (Working now):
- Graph-based reasoning with explainability âœ…
- Real-time learning without retraining âœ…
- Semantic similarity enhancement âœ…
- REST API âœ…

**Mid-term** (In progress):
- Replace LLM-style interfaces completely
- Streaming responses
- Multi-modal support (text + structured data)
- Distributed reasoning

**Long-term** (Research):
- Replace all black-box neural networks with explainable alternatives
- Provable correctness for critical decisions
- Zero-trust AI systems where every output is verifiable

## Performance Characteristics

Storage-server benchmarks (production):

- **Learning**: 0.02ms per concept (57,412/sec)
- **Query (read)**: <0.01ms via in-memory snapshot
- **Path finding**: ~1ms for 3-hop BFS (server-side)
- **Storage**: Single file, memory-mapped, lock-free writes
- **Vector search**: HNSW O(log N)

## Key Design Decisions

### Why Graph-Based?

Graphs are inherently explainable. You can trace every reasoning path. LLMs are not.

### Why Rust for Storage?

Python is great for logic but slow for I/O. Rust gives us:
- Zero-copy memory-mapped files
- Lock-free concurrency
- Predictable performance

### Why Optional Embeddings?

Pure graph reasoning is 100% explainable. Embeddings enhance it but add some opacity. We make it optional and always show contribution.

### Why REST API as Sole Interface?

Clean separation. Internal implementation can change without breaking users.

## Project Structure

```
sutra-models/
â”œâ”€â”€ packages/
â”‚   â”œâ”€â”€ sutra-core/          # Graph reasoning engine
â”‚   â”œâ”€â”€ sutra-storage/        # Rust storage backend  
â”‚   â”œâ”€â”€ sutra-hybrid/         # Semantic embeddings
â”‚   â””â”€â”€ sutra-api/            # REST API (FastAPI)
â”œâ”€â”€ test_direct_workflow.py   # End-to-end test
â”œâ”€â”€ test_api_workflow.py      # API integration test
â”œâ”€â”€ QUICK_START.md            # How to run and test
â””â”€â”€ README.md                 # This file
```

## Testing

```bash
# Test core package
make test-core

# Test end-to-end workflow (no API)
python test_direct_workflow.py

# Test API workflow (requires API server running)
python test_api_workflow.py

# Format code
make format

# Lint
make lint
```

## Configuration

Via environment variables or config files:

```bash
# Storage location
export SUTRA_STORAGE_PATH="./knowledge"

# Enable semantic embeddings
export SUTRA_USE_SEMANTIC_EMBEDDINGS="true"

# API settings
export SUTRA_API_PORT="8000"

# Rate limits
export SUTRA_RATE_LIMIT_LEARN="30"
export SUTRA_RATE_LIMIT_REASON="60"
```

## Dependencies

**Core**:
- Python 3.8+
- numpy
- sutra-storage (Rust, compiled to Python extension)

**Optional**:
- sentence-transformers (for semantic embeddings)
- spaCy (for enhanced NLP)
- FastAPI + uvicorn (for API server)

## What This Is Not

- **Not an LLM replacement yet** - We're working toward it, but not there yet
- **Not trained on massive datasets** - Learns from what you give it
- **Not a general knowledge base** - Specialized for your domain
- **Not "AI magic"** - Deterministic reasoning with explainable paths

## Current Limitations

Honest assessment of what doesn't work yet:

1. **Limited reasoning depth** - Works well for 2-3 hops, gets expensive beyond that
2. **No natural language generation** - Returns concept content, not fluent text
3. **Requires structured input** - Works best with clear factual statements
4. **No common sense reasoning** - Only knows what you teach it
5. **English-only** - NLP components are English-centric

## Contributing

We welcome contributions that align with the mission of explainable, accountable AI.

Before contributing:
1. Read the architecture docs in WARP.md
2. Run tests to verify your changes
3. Follow the existing code style (black + isort)
4. Add tests for new features

## Research Foundation

Built on published research:

- **Adaptive Focus Learning**: "LLM-Oriented Token-Adaptive Knowledge Distillation" (Oct 2024)
- **Multi-Path Plan Aggregation (MPPA)**: Consensus-based reasoning
- **Graph-based reasoning**: Decades of knowledge representation research

No proprietary "secret sauce" - all techniques are from published work.

## License

MIT License - see LICENSE file

## Contact

This is an active research project. We're figuring things out as we go.

Issues and pull requests welcome.

---

**Status**: Production-ready for internal use. API tested end-to-end. Full persistence verified.  
**Version**: 2.0.0  
**Last tested**: 2025-10-16
